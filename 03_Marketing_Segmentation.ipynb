{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38365d44",
      "metadata": {
        "id": "38365d44"
      },
      "source": [
        "# **Marketing Campaign Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70cedf1",
      "metadata": {
        "id": "b70cedf1"
      },
      "source": [
        "## Business Problem\n",
        "Businesses often struggle to personalize marketing efforts due to a lack of structured customer insights. Traditional one-size-fits-all approaches lead to inefficient resource allocation and missed opportunities. A data-driven segmentation approach enables more targeted engagement, improving overall customer satisfaction and business profitability. Effective segmentation leads to improved customer engagement, better-targeted marketing efforts, and higher return on investment (ROI). Research shows that segmented marketing campaigns significantly outperform non-segmented ones, leading to increased customer interactions and revenue. The goal is to identify distinct customer segmentations based on common characteristics to enable personalized marketing strategies.\n",
        "Key Questions:\n",
        "What distinct customer segments exist within the current customer base?\n",
        "How do demographic characteristics (e.g., income, age, family status) influence purchasing behavior?\n",
        "Which customer groups are most responsive to specific marketing campaigns?\n",
        "What marketing strategies will maximize ROI for each identified segment?\n",
        "How do online engagement patterns correlate with purchasing behaviors?\n",
        "Which clusters represent high-value customers versus price-sensitive buyers?\n",
        "The Data\n",
        "The dataset consists of 2240 rows and 27 columns.\n",
        "Out of 27 columns, 24 columns are numeric and 3 columns are of object data type.\n",
        "There are a few missing values in the Income variable that were fixed.\n",
        "The ID column is an identifier which is unique for each customer.\n",
        "Data was collected in 2016.\n",
        "\n",
        "Approach\n",
        "Exploratory Data Analysis (EDA): Understand the dataset, detect missing values, and identify patterns.\n",
        "Univariate and Bivariate Analysis: Examine individual and pairwise relationships between variables.\n",
        "Feature Engineering & Data Processing: Prepare and transform data for optimal clustering.\n",
        "Scaling: Normalize data to ensure consistent distance measurements for clustering algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd434483",
      "metadata": {
        "id": "bd434483"
      },
      "source": [
        "------------------------------\n",
        "## **Data Dictionary**\n",
        "------------------------------\n",
        "\n",
        "The dataset contains the following features:\n",
        "\n",
        "1. ID: Unique ID of each customer\n",
        "2. Year_Birth: Customer’s year of birth\n",
        "3. Education: Customer's level of education\n",
        "4. Marital_Status: Customer's marital status\n",
        "5. Kidhome: Number of small children in customer's household\n",
        "6. Teenhome: Number of teenagers in customer's household\n",
        "7. Income: Customer's yearly household income in USD\n",
        "8. Recency: Number of days since the last purchase\n",
        "9. Dt_Customer: Date of customer's enrollment with the company\n",
        "10. MntFishProducts: The amount spent on fish products in the last 2 years\n",
        "11. MntMeatProducts: The amount spent on meat products in the last 2 years\n",
        "12. MntFruits: The amount spent on fruits products in the last 2 years\n",
        "13. MntSweetProducts: Amount spent on sweet products in the last 2 years\n",
        "14. MntWines: The amount spent on wine products in the last 2 years\n",
        "15. MntGoldProds: The amount spent on gold products in the last 2 years\n",
        "16. NumDealsPurchases: Number of purchases made with discount\n",
        "17. NumCatalogPurchases: Number of purchases made using a catalog (buying goods to be shipped through the mail)\n",
        "18. NumStorePurchases: Number of purchases made directly in stores\n",
        "19. NumWebPurchases: Number of purchases made through the company's website\n",
        "20. NumWebVisitsMonth: Number of visits to the company's website in the last month\n",
        "21. AcceptedCmp1: 1 if customer accepted the offer in the first campaign, 0 otherwise\n",
        "22. AcceptedCmp2: 1 if customer accepted the offer in the second campaign, 0 otherwise\n",
        "23. AcceptedCmp3: 1 if customer accepted the offer in the third campaign, 0 otherwise\n",
        "24. AcceptedCmp4: 1 if customer accepted the offer in the fourth campaign, 0 otherwise\n",
        "25. AcceptedCmp5: 1 if customer accepted the offer in the fifth campaign, 0 otherwise\n",
        "26. Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n",
        "27. Complain: 1 If the customer complained in the last 2 years, 0 otherwise\n",
        "\n",
        "**Note:** You can assume that the data is collected in the year 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vEUKOiz5-Qld",
      "metadata": {
        "id": "vEUKOiz5-Qld"
      },
      "source": [
        "### **Loading Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4297cf5d",
      "metadata": {
        "id": "4297cf5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f50fcb0-3adf-47dc-a692-7c0cce44267e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.5.0)\n",
            "Downloading scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.3.0\n"
          ]
        }
      ],
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To scale the data using z-score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# To compute distances\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# To perform K-means clustering and compute Silhouette scores\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# To visualize the elbow curve and Silhouette scores\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# Importing PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# To encode the variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Importing TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "\n",
        "# To compute distances\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "\n",
        "# Install scikit-learn-extra\n",
        "!pip install scikit-learn-extra\n",
        "\n",
        "# To import K-Medoids\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "# To import Gaussian Mixture\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# To supress warnings\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZAEqtrQu-VzO",
      "metadata": {
        "id": "ZAEqtrQu-VzO"
      },
      "source": [
        "### **Let us load the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5GMyL7X0hRj",
        "outputId": "e739071e-f51c-406e-99c1-9237da5612d3"
      },
      "id": "T5GMyL7X0hRj",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b39cc0",
      "metadata": {
        "id": "70b39cc0"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Data Science MIT Cert /Capstone Project/marketing_campaign.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e64e53a",
      "metadata": {
        "id": "3e64e53a"
      },
      "source": [
        "### **Check the shape of the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the data\n",
        "data.info()"
      ],
      "metadata": {
        "id": "6-Tx4brh32yB"
      },
      "id": "6-Tx4brh32yB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "O9rtKHxB-fy1",
      "metadata": {
        "id": "O9rtKHxB-fy1"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "* 27 colums: want to make some reductions\n",
        "* Can delete ID # probs\n",
        "* 3 are object types: Education, maritial status, DTcostumer (this will need to be addressed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BoKgZyQ-nAF",
      "metadata": {
        "id": "6BoKgZyQ-nAF"
      },
      "source": [
        "### **Understand the data by observing a few rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd1bc4b",
      "metadata": {
        "id": "9cd1bc4b"
      },
      "outputs": [],
      "source": [
        "# View first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94b4947",
      "metadata": {
        "id": "e94b4947"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c417a660",
      "metadata": {
        "id": "c417a660"
      },
      "outputs": [],
      "source": [
        "# View last 5 rows Hint: Use tail() method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "gPye_QY848tm"
      },
      "id": "gPye_QY848tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e18d7cee",
      "metadata": {
        "id": "e18d7cee"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "* Education: PHD, Graduation, Master\n",
        "* Marital_Status: Married, together, divorced, single, Widowed?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c9e095",
      "metadata": {
        "id": "b4c9e095"
      },
      "source": [
        "### **Let us check the data types and and missing values of each column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14a018a",
      "metadata": {
        "id": "c14a018a"
      },
      "outputs": [],
      "source": [
        "# Check the datatypes of each column. Hint: Use info() method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "eLm9j-WR5yKD"
      },
      "id": "eLm9j-WR5yKD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1e81dd",
      "metadata": {
        "id": "5b1e81dd"
      },
      "outputs": [],
      "source": [
        "# Find the percentage of missing values in each column of the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()"
      ],
      "metadata": {
        "id": "foq4ToEM52_U"
      },
      "id": "foq4ToEM52_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ae64b672",
      "metadata": {
        "id": "ae64b672"
      },
      "source": [
        "#### **Observations and Insights: _____**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375397e7",
      "metadata": {
        "id": "375397e7"
      },
      "source": [
        "We can observe that `ID` has no null values. Also the number of unique values are equal to the number of observations. So, `ID` looks like an index for the data entry and such a column would not be useful in providing any predictive power for our analysis. Hence, it can be dropped."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0d_Sclu-Wyu2"
      },
      "id": "0d_Sclu-Wyu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a01418e2",
      "metadata": {
        "id": "a01418e2"
      },
      "source": [
        "**Dropping the ID column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b25be10",
      "metadata": {
        "id": "3b25be10"
      },
      "outputs": [],
      "source": [
        "# Remove ID column from data. Hint: Use inplace = True\n",
        "data.drop(columns = ['ID'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "gU8J4K0H7wCa"
      },
      "id": "gU8J4K0H7wCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "835c10c4",
      "metadata": {
        "id": "835c10c4"
      },
      "source": [
        "## **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e047d85",
      "metadata": {
        "id": "0e047d85"
      },
      "source": [
        "### **Let us now explore the summary statistics of numerical variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ceba1e",
      "metadata": {
        "id": "c9ceba1e"
      },
      "outputs": [],
      "source": [
        "# Explore basic summary statistics of numeric variables. Hint: Use describe() method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().T"
      ],
      "metadata": {
        "id": "whoV2MoO7Li1"
      },
      "id": "whoV2MoO7Li1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "63fa40ad",
      "metadata": {
        "id": "63fa40ad"
      },
      "source": [
        "### **Let us also explore the summary statistics of all categorical variables and the number of unique observations in each category**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac583e5c",
      "metadata": {
        "id": "ac583e5c"
      },
      "outputs": [],
      "source": [
        "# List of the categorical columns in the data\n",
        "cols = [\"Education\", \"Marital_Status\", \"Kidhome\", \"Teenhome\", \"Complain\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b8b46e",
      "metadata": {
        "id": "45b8b46e"
      },
      "source": [
        "**Number of unique observations in each category**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51adf97",
      "metadata": {
        "id": "c51adf97"
      },
      "outputs": [],
      "source": [
        "for column in cols:\n",
        "    print(\"Unique values in\", column, \"are :\")\n",
        "    print(data[column].unique())\n",
        "    print(\"*\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b75da6",
      "metadata": {
        "id": "73b75da6"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations:\n",
        "\n",
        "* Income:\n",
        "The average income of customers is around 52,247.\n",
        " The minimum income is 1,730 and the maximum is 666,666.\n",
        "There's a significant difference between the mean and the median (50th percentile) of income, indicating a skewed distribution with potential outliers.\n",
        "\n",
        "* Age:\n",
        "The average age of customers is around 46.\n",
        "The minimum age is 18 and the maximum is 128 (this might indicate incorrect or outlier data).\n",
        "The distribution of age seems to be slightly skewed towards younger customers as the mean is lower than the median.\n",
        "\n",
        "* Recency:\n",
        "On average, the last purchase was made around 50 days ago.\n",
        "The recency values range from 0 to 99, suggesting that some customers have made recent purchases, while others haven't purchased in almost 100 days.\n",
        "\n",
        "* Mnt (Amount spent on products):\n",
        "The average amount spent on most products (wines, fruits, meat, fish, sweet products, gold products) is in the tens or hundreds of dollars, suggesting that customers generally purchase these products regularly.\n",
        "There is variability in the amounts spent on different products, indicating diverse preferences among customers.\n",
        "\n",
        "* Num (Number of purchases):\n",
        "The average number of purchases made through different channels (deals, catalog, store, web) is relatively low.\n",
        "The minimum value for most of these variables is 0, suggesting that some customers haven't used certain channels for purchases.\n",
        "\n",
        "* NumWebVisitsMonth:\n",
        "Customers visit the company's website an average of 5 times per month.\n",
        "The minimum value is 0, indicating that some customers haven't visited the website recently.\n",
        "\n",
        "* AcceptedCmp:\n",
        "The percentage of customers who accepted offers in previous campaigns (Cmp1 to Cmp5) is generally low, ranging from around 7% to 15%.\n",
        "This suggests that most customers don't often respond positively to marketing campaigns.\n",
        "\n",
        "* Response:\n",
        "Around 15% of the customers responded to the last campaign.\n",
        "\n",
        "* Complain:\n",
        "Only around 3% of the customers have complained in the last 2 years, suggesting overall customer satisfaction.\n",
        "\n",
        "## Insights:\n",
        "\n",
        "* The dataset contains customers with diverse income levels, age groups, and purchasing behaviors.\n",
        "There might be outliers in the income and age variables that need further investigation.\n",
        "Customer engagement with different marketing channels varies, with some channels being more preferred than others.\n",
        "\n",
        "* The company's marketing campaigns have had limited success in the past, as evidenced by the low acceptance rates.\n",
        "\n",
        "* Most customers seem to be generally satisfied, as indicated by the low complaint rate.\n",
        "These observations and insights should be followed up with more in-depth analysis to develop a deeper understanding of the data and derive actionable insights for marketing strategies. I hope this helps! Let me know if you have any other questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0df3d2",
      "metadata": {
        "id": "5e0df3d2"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- We could observe from the summary statistics of categorical variables that the Education variable has 5 categories. Are all categories different from each other or can we combine some categories? Is 2n Cycle different from Master?\n",
        "- Similarly, there are 8 categories in Marital_Status with some categories having very low count of less than 5. Can we combine these categories with other categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c1bd02",
      "metadata": {
        "id": "f7c1bd02"
      },
      "source": [
        "### **Let us replace  the \"2n Cycle\" category with \"Master\" in Education and \"Alone\", \"Absurd, and \"YOLO\" with \"Single\" in Marital_Status**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd493443",
      "metadata": {
        "id": "fd493443"
      },
      "outputs": [],
      "source": [
        "# Replace the category \"2n Cycle\" with the category \"Master\"\n",
        "\n",
        "data[\"Education\"].replace({\"2n Cycle\": \"Master\"}, inplace=True) # Hint: Use the replace() method and inplace=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3e1ae4",
      "metadata": {
        "id": "de3e1ae4"
      },
      "outputs": [],
      "source": [
        "# Replace the categories \"Alone\", \"Abusrd\", \"YOLO\" with the category \"Single\"\n",
        "\n",
        "data[\"Marital_Status\"].replace({\"Alone\": \"Single\", \"Absurd\": \"Single\", \"YOLO\": \"Single\"}, inplace=True)  # Hint: Use the replace() method and inplace=True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356c9bc7",
      "metadata": {
        "id": "356c9bc7"
      },
      "source": [
        "## **Univariate Analysis**\n",
        "Univariate analysis is used to explore each variable in a data set, separately. It looks at the range of values, as well as the central tendency of the values. It can be done for both numerical and categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b75f017",
      "metadata": {
        "id": "3b75f017"
      },
      "source": [
        "## **1. Univariate Analysis - Numerical Data**\n",
        "Histograms help to visualize and describe numerical data. We can also use other plots like box plot to analyze the numerical columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kY9pS6xKQkRA",
      "metadata": {
        "id": "kY9pS6xKQkRA"
      },
      "source": [
        "#### Let us plot histogram for the feature 'Income' to understand the distribution and outliers, if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b313c47f",
      "metadata": {
        "id": "b313c47f"
      },
      "outputs": [],
      "source": [
        "# Create histogram for the Income feature\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'Income', data=data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vshFcmLmBRYf"
      },
      "id": "vshFcmLmBRYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e804630f",
      "metadata": {
        "id": "e804630f"
      },
      "source": [
        "**We could observe some extreme value on the right side of the distribution of the 'Income' feature. Let's use a box plot as it is more suitable to identify extreme values in the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a79ed8f",
      "metadata": {
        "id": "0a79ed8f"
      },
      "outputs": [],
      "source": [
        "# Plot the boxplot\n",
        "sns.boxplot(data=data, x= \"Income\", showmeans=True, color=\"violet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9220096a",
      "metadata": {
        "id": "9220096a"
      },
      "source": [
        "#### **Observations and Insights:\n",
        "There is an outlier very far right. This is quite skewed.\n",
        "I decided to keep this outlier as hight income is realistic to customers. There are less people with extremely high income so this tracks. If I were to remove this outlier than I would overlook and important coustumer base."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c75b036",
      "metadata": {
        "id": "4c75b036"
      },
      "source": [
        "- The histogram and the box plot are showing some extreme value on the right side of the distribution of the 'Income' feature. Can we consider them as outliers and remove or should we analyze these extreme values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1f2aeb",
      "metadata": {
        "id": "2d1f2aeb"
      },
      "outputs": [],
      "source": [
        "# Calculating the upper whisker for the Income variable\n",
        "\n",
        "Q1 = data[\"Income\"].quantile(q=0.25)                          # Finding the first quartile\n",
        "\n",
        "Q3 = data[\"Income\"].quantile(q=0.75)                        # Finding the third quartile\n",
        "\n",
        "IQR = Q3 - Q1                                  # Finding the Inter Quartile Range\n",
        "\n",
        "upper_whisker = (Q3 + 1.5 * IQR)          # Calculating the Upper Whisker for the Income variable\n",
        "\n",
        "print(upper_whisker)                                # Printing Upper Whisker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74c8074",
      "metadata": {
        "id": "e74c8074"
      },
      "outputs": [],
      "source": [
        "# Let's check the observations with extreme value for the Income variable\n",
        "data[data.Income > upper_whisker]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e28a104",
      "metadata": {
        "id": "4e28a104"
      },
      "source": [
        "\n",
        "- We observed that there are only a few rows with extreme values for the Income variable. Is that enough information to treat (or not to treat) them? Do we know at what percentile the upper whisker lies?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the 99.5% percentile value for the Income variable\n",
        "\n",
        "# Calculate the 99.5th percentile of the 'Income' variable\n",
        "percentile_99_5 = data['Income'].quantile(0.995)\n",
        "\n",
        "print(f\"The 99.5th percentile value for Income is: {percentile_99_5}\")\n"
      ],
      "metadata": {
        "id": "nVZ-92w10dCV"
      },
      "id": "nVZ-92w10dCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5d8c69dd",
      "metadata": {
        "id": "5d8c69dd"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "* Need to drop outliers beyond 99.5th percentile becuase it is skewing data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping observations identified as outliers\n",
        "# Get the indices of rows where 'Income' exceeds the 99.5th percentile\n",
        "outlier_indices = data[data['Income'] > percentile_99_5].index\n",
        "\n",
        "# Drop those rows from the DataFrame\n",
        "data.drop(index=outlier_indices, inplace=True)"
      ],
      "metadata": {
        "id": "N1_Wa_yZ1Z7M"
      },
      "id": "N1_Wa_yZ1Z7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e7c1f6",
      "metadata": {
        "id": "a9e7c1f6"
      },
      "outputs": [],
      "source": [
        "# Dropping observations identified as outliers\n",
        "#data.drop(index=[data['Income']], inplace=True) # Pass the indices of the observations (separated by a comma) to drop them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af00faa6",
      "metadata": {
        "id": "af00faa6"
      },
      "source": [
        "**Now, let's check the distribution of the Income variable after dropping outliers.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04fc8635",
      "metadata": {
        "id": "04fc8635"
      },
      "outputs": [],
      "source": [
        "# Plot histogram and 'Income'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'Income', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8rd-KNbWEhyf"
      },
      "id": "8rd-KNbWEhyf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8d965d",
      "metadata": {
        "id": "9e8d965d"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntWines'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntWines', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZqJTMd7ElPS"
      },
      "id": "SZqJTMd7ElPS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a620df6f",
      "metadata": {
        "id": "a620df6f"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntFruits'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntFruits', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f-8V0nxCEr7H"
      },
      "id": "f-8V0nxCEr7H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e3cb1c",
      "metadata": {
        "id": "b9e3cb1c"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntMeatProducts'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntMeatProducts', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cw7YmOXrEwiQ"
      },
      "id": "cw7YmOXrEwiQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a279b68",
      "metadata": {
        "id": "6a279b68"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntFishProduct'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntFishProducts', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CHGHnYfFE2KI"
      },
      "id": "CHGHnYfFE2KI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68acbdb6",
      "metadata": {
        "id": "68acbdb6"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntSweetProducts'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntSweetProducts', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NnUEcw2fFLzM"
      },
      "id": "NnUEcw2fFLzM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea5701e",
      "metadata": {
        "id": "4ea5701e"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for 'MntGoldProducts'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.histplot(x= 'MntGoldProds', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m7LnZOMgFR8d"
      },
      "id": "m7LnZOMgFR8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ac04d87d",
      "metadata": {
        "id": "ac04d87d"
      },
      "source": [
        "## **2. Univariate analysis - Categorical Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c6025d",
      "metadata": {
        "id": "d3c6025d"
      },
      "source": [
        "Let us write a function that will help us create bar plots that indicate the percentage for each category. This function takes the categorical column as the input and returns the bar plot for the variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'Marital_Status'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAo2moSZ2vLW"
      },
      "id": "nAo2moSZ2vLW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'Education'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gysi9KQi3fA_"
      },
      "id": "Gysi9KQi3fA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'Kidhome'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fl2hoxBy3kmj"
      },
      "id": "fl2hoxBy3kmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Teenhome\n",
        "z = 'Teenhome'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdZjrYdV3s21"
      },
      "id": "JdZjrYdV3s21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "z = 'Complain'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GWixLHZ2GW66"
      },
      "id": "GWixLHZ2GW66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'Response'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fpEYVuvD4Fwk"
      },
      "id": "fpEYVuvD4Fwk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'AcceptedCmp1'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jQkHpIRM4T_L"
      },
      "id": "jQkHpIRM4T_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'AcceptedCmp2'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YlXtHYhY4ZfK"
      },
      "id": "YlXtHYhY4ZfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'AcceptedCmp3'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7HUifzS34dRB"
      },
      "id": "7HUifzS34dRB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'AcceptedCmp4'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tOEYimiD4hOb"
      },
      "id": "tOEYimiD4hOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = 'AcceptedCmp5'\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)\n",
        "total = len(data[z])  # Total number of observations\n",
        "\n",
        "for p in ax.patches:\n",
        "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)  # Calculate percentage\n",
        "    x = p.get_x() + p.get_width() / 2 - 0.05  # X position for annotation\n",
        "    y = p.get_y() + p.get_height()            # Y position for annotation\n",
        "\n",
        "    ax.annotate(percentage, (x, y), size=12, ha='center')  # Annotate with percentage\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tzZDvYIn4mq8"
      },
      "id": "tzZDvYIn4mq8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "45e70646",
      "metadata": {
        "id": "45e70646"
      },
      "source": [
        "#### **Note:** Explore for other categorical variables like Education, Kidhome, Teenhome, Complain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4989521",
      "metadata": {
        "id": "b4989521"
      },
      "source": [
        "#### **Observations and Insights from all plots:**\n",
        "Numerical Features:\n",
        "\n",
        "Income:\n",
        "The distribution is right-skewed, indicating a few customers with very high incomes.\n",
        "There are outliers present in the data, which were identified using the boxplot and the IQR method.\n",
        "The outliers beyond the 99.5th percentile were removed to reduce the skewness.\n",
        "MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds:\n",
        "These features represent the amount spent on different product categories.\n",
        "All of these distributions are right-skewed, indicating that most customers spend a moderate amount on these products, while a few spend significantly more.\n",
        "Categorical Features:\n",
        "\n",
        "Marital_Status:\n",
        "Most customers are married or in a relationship (\"Relationship\" category).\n",
        "A smaller percentage of customers are single.\n",
        "Education:\n",
        "The majority of customers have a Graduation degree.\n",
        "A significant portion also have a Master's or PhD degree.\n",
        "Kidhome, Teenhome:\n",
        "Most customers have 0 or 1 kid at home.\n",
        "Most customers have 0 or 1 teen at home.\n",
        "Complain:\n",
        "A very small percentage of customers have complained in the last 2 years.\n",
        "Response:\n",
        "A small percentage of customers responded to the last campaign.\n",
        "AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5:\n",
        "These features indicate whether a customer accepted an offer in a previous campaign.\n",
        "For each campaign, the majority of customers did not accept the offer.\n",
        "Overall Insights:\n",
        "\n",
        "The dataset contains customers with diverse income levels, purchasing behaviors, and family structures.\n",
        "There are potential outliers in the income and spending variables that require further investigation.\n",
        "Customer engagement with marketing campaigns has been relatively low in the past.\n",
        "Most customers seem to be generally satisfied, as indicated by the low complaint rate.\n",
        "Recommendations:\n",
        "\n",
        "Consider further investigating the outliers and their potential impact on the analysis.\n",
        "Explore customer segmentation to identify groups with different purchasing patterns and tailor marketing strategies accordingly.\n",
        "Analyze the factors influencing campaign acceptance to improve future campaign performance.\n",
        "I hope this helps! Let me know if you have any other questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818f6411",
      "metadata": {
        "id": "818f6411"
      },
      "source": [
        "## **Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e50dd4",
      "metadata": {
        "id": "64e50dd4"
      },
      "source": [
        "We have analyzed different categorical and numerical variables. Now, let's check how different variables are related to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef227873",
      "metadata": {
        "id": "ef227873"
      },
      "source": [
        "### **Correlation Heat map**\n",
        "Heat map can show a 2D correlation matrix between numerical features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))                                                        # Setting the plot size\n",
        "sns.heatmap(data.select_dtypes(include=np.number).corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")  # Plotting the correlation plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "thUvMWpG7z0H"
      },
      "id": "thUvMWpG7z0H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd9ab5a",
      "metadata": {
        "id": "5bd9ab5a"
      },
      "outputs": [],
      "source": [
        "#plt.figure(figsize=(15, 7))                                                        # Setting the plot size\n",
        "#sns.heatmap(___________, annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")  # Plotting the correlation plot\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a57dd0a",
      "metadata": {
        "id": "7a57dd0a"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations\n",
        "\n",
        "Strong Positive Correlations: There are strong positive correlations between the following pairs of variables:\n",
        "NumWebPurchases and NumWebVisitsMonth: This suggests that customers who visit the company's website more often also tend to make more purchases online.\n",
        "MntWines and Expenses: This is expected as wine purchases contribute to the overall expenses. Similar relationships are observed for other product categories (MntFruits, MntMeatProducts, etc.) and Expenses.\n",
        "NumCatalogPurchases and Expenses: Customers who make more catalog purchases also tend to have higher overall expenses.\n",
        "NumStorePurchases and Expenses: Customers who make more in-store purchases also tend to have higher overall expenses.\n",
        "NumDealsPurchases and NumCatalogPurchases: This suggests that customers who buy products using catalogs are more likely to take advantage of deals or promotions.\n",
        "Moderate Positive Correlations: There are moderate positive correlations between variables such as MntWines and MntMeatProducts, indicating that customers who purchase more wine also tend to purchase more meat products. Similar patterns might be observed for other combinations of product categories.\n",
        "Weak or No Correlations: Most other variable pairs show weak or no correlations, suggesting that they are relatively independent of each other. For example, Recency and Income appear to have a very weak correlation.\n",
        "Insights\n",
        "\n",
        "Customer Segmentation: The correlations between purchasing behaviors (e.g., NumWebPurchases, NumCatalogPurchases, MntWines, etc.) suggest that customers can be segmented based on their preferences for different product categories and purchasing channels.\n",
        "Targeted Marketing: The positive correlations between NumWebPurchases, NumWebVisitsMonth, and Expenses imply that increasing website traffic and engagement could lead to higher sales. Marketing efforts could focus on attracting customers to the website and encouraging online purchases.\n",
        "Product Bundling: The moderate positive correlations between different product categories suggest that offering product bundles or promotions that combine related products could be effective in increasing sales. For example, bundling wine and meat products could appeal to customers who purchase both regularly.\n",
        "Channel Optimization: The correlations between purchasing channels and expenses indicate that optimizing each channel (e.g., website, catalog, in-store) to cater to customer preferences could lead to higher overall spending."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c00607",
      "metadata": {
        "id": "16c00607"
      },
      "source": [
        "**The above correlation heatmap only shows the relationship between numerical variables. Let's check the relationship of numerical variables with categorical variables.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d99eb8",
      "metadata": {
        "id": "27d99eb8"
      },
      "source": [
        "### **Education Vs Income**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.barplot(x='Education', y='Income', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XUMLdLX58erb"
      },
      "id": "XUMLdLX58erb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0bbdb8f0",
      "metadata": {
        "id": "0bbdb8f0"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations:\n",
        "\n",
        "Customers with a PhD tend to have the highest average income.\n",
        "Customers with a Master's degree have the second-highest average income, slightly lower than those with a PhD.\n",
        "Customers with a Graduation degree have a moderate average income, lower than those with Master's or PhD degrees.\n",
        "Customers with a Basic degree have the lowest average income.\n",
        "Insights:\n",
        "\n",
        "There is a positive correlation between education level and income. Higher education levels are generally associated with higher incomes.\n",
        "This suggests that customers with higher education levels may have greater purchasing power and could be more receptive to marketing campaigns for premium products or services.\n",
        "The company could consider tailoring its marketing strategies to target specific education segments with products and messaging that resonate with their income levels and preferences.\n",
        "Customers with lower education levels might require different approaches and may be more price-sensitive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9695eb",
      "metadata": {
        "id": "1c9695eb"
      },
      "source": [
        "### **Marital Status Vs Income**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151b12ca",
      "metadata": {
        "id": "151b12ca"
      },
      "outputs": [],
      "source": [
        "# Plot the bar plot for Marital_Status and Income"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.barplot(x='Marital_Status', y='Income', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ehU3NA1M8nJx"
      },
      "id": "ehU3NA1M8nJx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e888fe34",
      "metadata": {
        "id": "e888fe34"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "\n",
        "Okay, let's analyze the observations and insights from the \"Marital Status vs. Income\" plot:\n",
        "\n",
        "Observations:\n",
        "\n",
        "Income Variation Across Marital Statuses: There is noticeable variation in average income levels among different marital statuses.\n",
        "Highest Income Group: Customers who are \"single\" tend to have the highest average income.\n",
        "Lowest Income Group: Customers who are \"widow\" or \"divorced\" appear to have lower average incomes.\n",
        "Married and Together: Customers who are \"married\" or in a \"relationship\" have similar average income levels, falling in between the highest and lowest income groups.\n",
        "Insights:\n",
        "\n",
        "Targeting Single Customers: Given the higher average income of single customers, marketing campaigns could focus on this group with products and services tailored to their interests and preferences.\n",
        "Considerations for Widowed and Divorced: The lower income levels for widowed and divorced individuals might suggest the need for different marketing approaches. These segments may be more price-sensitive or require targeted products catering to their specific needs.\n",
        "Income and Family Structure: Income levels appear to be influenced by family structure and marital status. Considering these factors in market segmentation strategies could lead to more effective targeting and messaging.\n",
        "Further Investigation: The observed income variation among marital statuses warrants further investigation to identify potential underlying reasons and opportunities for customized marketing efforts.\n",
        "Additional considerations:\n",
        "\n",
        "While the plot shows average income for each group, there is likely income variability within each group. Further analysis (e.g., using box plots) could help in understanding the distribution and identifying potential outliers.\n",
        "The data may not capture all the relevant factors influencing income differences between groups. There could be other demographic or socioeconomic variables impacting the relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbeafd2d",
      "metadata": {
        "id": "bbeafd2d"
      },
      "source": [
        "### **Kidhome Vs Income**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378353ea",
      "metadata": {
        "id": "378353ea"
      },
      "outputs": [],
      "source": [
        "# Plot the bar plot for Kidhome and Income"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.barplot(x='Kidhome', y='Income', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ScXyMzst883h"
      },
      "id": "ScXyMzst883h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6515b240",
      "metadata": {
        "id": "6515b240"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations:\n",
        "\n",
        "Income and Number of Kids at Home: There's a clear relationship between the number of kids at home (Kidhome) and the average income of customers.\n",
        "Highest Income: Customers with 0 kids at home tend to have the highest average income.\n",
        "Decreasing Income with More Kids: As the number of kids at home increases (from 1 to 2), the average income generally decreases.\n",
        "Insights:\n",
        "\n",
        "Household Expenses and Income: The presence of kids at home is likely associated with higher household expenses, which might impact the overall income available for discretionary spending.\n",
        "Targeting Customers with No Kids: Marketing campaigns could focus on customers with no kids at home, as they tend to have higher disposable income and might be more receptive to premium products or services.\n",
        "Tailored Strategies for Families with Kids: For customers with kids at home, marketing strategies could consider their specific needs and preferences, potentially focusing on family-oriented products or value-driven offerings.\n",
        "Further Segmentation: It's worth exploring further segmentation within the Kidhome categories to understand potential nuances in purchasing behavior and income levels. For example, families with young children might have different needs compared to those with teenagers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e490deeb",
      "metadata": {
        "id": "e490deeb"
      },
      "source": [
        "**We can also visualize the relationship between two categorical variables.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0650b3c1",
      "metadata": {
        "id": "0650b3c1"
      },
      "source": [
        "### **Marital_Status Vs Kidhome**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the bar plot for Marital_Status and Kidhome\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.countplot(x='Marital_Status', hue='Kidhome', data=data)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7-oR7tKxifFc"
      },
      "id": "7-oR7tKxifFc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a631f993",
      "metadata": {
        "id": "a631f993"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations:\n",
        "\n",
        "Relationship Status and Kids: There's a noticeable relationship between marital status and the number of kids at home.\n",
        "\n",
        "Married/Together with Kids: A large portion of married or \"together\" customers have kids (either Kidhome=1 or Kidhome=2). This aligns with expectations, as these relationship statuses often involve family structures.\n",
        "\n",
        "Single with No Kids: A significant portion of single customers have no kids at home (Kidhome=0). This suggests that a considerable number of single individuals in the dataset may not have children or have grown-up children who have moved out.\n",
        "\n",
        "Divorced/Widow: Divorced or widowed customers tend to have a mix of Kidhome values, with some having kids and others having none.\n",
        "\n",
        "Kids and Relationships: The presence of kids at home is more common among married or \"together\" customers compared to single, divorced, or widowed customers.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Family Structures: The plot provides insights into the family structures within the customer base. Married/together customers are more likely to have kids at home, while single customers are more likely to have no kids at home.\n",
        "\n",
        "Marketing Segmentation: These observations can be used for marketing segmentation. For instance, campaigns targeting families with kids could focus on married/together customers, while campaigns for products or services relevant to individuals without kids could target single customers.\n",
        "\n",
        "Product Development: Understanding the family structures of different customer segments can help in developing products or services tailored to their specific needs. For example, family-sized packages or products for kids could be targeted towards married/together customers.\n",
        "\n",
        "Lifestyle and Preferences: The relationship between marital status and Kidhome can provide insights into the lifestyles and preferences of different customer groups. For example, single customers may have more disposable income and may be more inclined towards travel or entertainment products, while families with kids might prioritize household goods or educational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e81e35d",
      "metadata": {
        "id": "7e81e35d"
      },
      "source": [
        "## **Feature Engineering and Data Processing**\n",
        "\n",
        "In this section, we will first prepare our dataset for analysis.\n",
        "- Creating new columns\n",
        "- Imputing missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68f9c28",
      "metadata": {
        "id": "d68f9c28"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- The Year_Birth column in the current format might not be very useful in our analysis. The Year_Birth column contains the information about Day, Month, and year. Can we extract the age of each customer?\n",
        "- Are there other columns which can be used to create new features?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5146558a",
      "metadata": {
        "id": "5146558a"
      },
      "source": [
        "### **Age**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the age by subtracting year_birth from 2016\n",
        "\n",
        "# Calculate the age by subtracting year_birth from 2016\n",
        "data['Age'] = 2016 - data['Year_Birth']\n"
      ],
      "metadata": {
        "id": "WBSxI5SSrfi7"
      },
      "id": "WBSxI5SSrfi7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print age of customers\n",
        "\n",
        "print(data['Age'])\n"
      ],
      "metadata": {
        "id": "3SykgXhOrq93"
      },
      "id": "3SykgXhOrq93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a copy of data frame and call it data2\n",
        "\n",
        "data2 = data.copy()\n"
      ],
      "metadata": {
        "id": "HnrHq3ZVrzAD"
      },
      "id": "HnrHq3ZVrzAD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From data2 replace year_birth with age\n",
        "\n",
        "# Assuming 'data' is your DataFrame and 'data2' is a copy as shown in your code.\n",
        "# Calculate the age and replace 'Year_Birth' with 'Age' in data2\n",
        "\n",
        "data2['Age'] = 2016 - data2['Year_Birth']\n",
        "data2 = data2.drop('Year_Birth', axis=1)\n",
        "\n",
        "print(data2['Age'])\n"
      ],
      "metadata": {
        "id": "I-rhW5bDr6zo"
      },
      "id": "I-rhW5bDr6zo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of data2\n",
        "\n",
        "import numpy as np\n",
        "# Assuming 'data2' is your DataFrame.  Replace this with your actual DataFrame variable if different.\n",
        "#  The code below provides a summary, including descriptive statistics for numerical columns and value counts for categorical ones.\n",
        "\n",
        "\n",
        "# Numerical Features Summary\n",
        "numerical_features = data2.select_dtypes(include=np.number).columns\n",
        "print(\"Summary of Numerical Features:\")\n",
        "print(data2[numerical_features].describe())\n",
        "\n",
        "# Categorical Features Summary\n",
        "categorical_features = data2.select_dtypes(exclude=np.number).columns\n",
        "print(\"\\nSummary of Categorical Features:\")\n",
        "for col in categorical_features:\n",
        "    print(f\"\\nValue Counts for '{col}':\")\n",
        "    print(data2[col].value_counts())\n"
      ],
      "metadata": {
        "id": "l5PJLYSKsJPh"
      },
      "id": "l5PJLYSKsJPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "EbzlByv5sUk7"
      },
      "id": "EbzlByv5sUk7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a bar graph to visual ages\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and 'Age' is a column in it\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Age', data=data2)\n",
        "plt.title('Distribution of Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UfSUem_tsf6u"
      },
      "id": "UfSUem_tsf6u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the range of ages?\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and 'Age' is a column in it\n",
        "print(f\"The range of ages is from {data2['Age'].min()} to {data2['Age'].max()}\")\n"
      ],
      "metadata": {
        "id": "0rPdcLHmsryz"
      },
      "id": "0rPdcLHmsryz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1d7ee7a7",
      "metadata": {
        "id": "1d7ee7a7"
      },
      "source": [
        "#### **Observations and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859d00a3",
      "metadata": {
        "id": "859d00a3"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- We could observe from the above output that there are customers with an age greater than 115. Can this be true or a data anomaly? Can we drop these observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcfa78f",
      "metadata": {
        "id": "8dcfa78f"
      },
      "outputs": [],
      "source": [
        "# Drop the observations with age > 115\n",
        "# Hint: Use drop() method with inplace=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop ages older than 115\n",
        "\n",
        "# Drop observations where age is greater than 115\n",
        "data2.drop(data2[data2['Age'] > 115].index, inplace=True)\n"
      ],
      "metadata": {
        "id": "Rphc-aYFs1yJ"
      },
      "id": "Rphc-aYFs1yJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The range of ages is from {data2['Age'].min()} to {data2['Age'].max()}\")"
      ],
      "metadata": {
        "id": "XEz3XvPKtKif"
      },
      "id": "XEz3XvPKtKif",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00e0e99e",
      "metadata": {
        "id": "00e0e99e"
      },
      "source": [
        "**Now, let's check the distribution of age in the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39316fd6",
      "metadata": {
        "id": "39316fd6"
      },
      "outputs": [],
      "source": [
        "# Plot histogram to check the distribution of age"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and 'Age' is a column in it\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Age', data=data2)\n",
        "plt.title('Distribution of Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OARqzSGntCk1"
      },
      "id": "OARqzSGntCk1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot of age\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and 'Age' is a column in it\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Age', data=data2)\n",
        "plt.title('Box Plot of Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FhSPxaAktN3R"
      },
      "id": "FhSPxaAktN3R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a0a4fc68",
      "metadata": {
        "id": "a0a4fc68"
      },
      "source": [
        "#### **Observations and Insights:**\n",
        "Observations:\n",
        "\n",
        "Age Calculation and Range: Customer age was derived by subtracting the Year_Birth from 2016 (assumed as the data collection year). The initial age range was observed to be from 18 to 128 years.\n",
        "\n",
        "Outliers: Anomalies were identified where customer ages exceeded 115 years. These were considered potential data errors and subsequently removed to ensure data accuracy.\n",
        "\n",
        "Age Distribution: After outlier removal, the age distribution was visualized using a histogram. The distribution revealed a concentration of customers within the age range of 30-60, with a slightly right-skewed tendency, indicating more customers in the younger to middle-aged groups.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Customer Demographics: The data suggests a focus on a customer base primarily within the younger to middle-aged demographic.\n",
        "\n",
        "Marketing and Product Development: Understanding the age distribution provides valuable input for targeted marketing and product development strategies. Focusing on the needs and preferences of the predominant age groups can lead to enhanced customer engagement and sales.\n",
        "\n",
        "Segmentation Opportunities: Age can serve as a segmentation variable, allowing for personalized messaging and promotions tailored to specific age brackets.\n",
        "\n",
        "Data Cleaning: The outlier detection and removal process highlighted the importance of data cleaning to ensure accurate and reliable analysis.\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "Age bands can be created to group customers into different age categories, which can facilitate more specific targeting and insights.\n",
        "\n",
        "Age can be combined with other demographic variables (like income, education, and family structure) to understand more nuanced customer segments and preferences.\n",
        "\n",
        "Exploring the relationship between age and other variables (like purchasing behavior, campaign responses, and product preferences) can offer valuable insights for developing customized marketing strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd31628",
      "metadata": {
        "id": "4bd31628"
      },
      "source": [
        "### **Kids**\n",
        "* Let's create feature \"Kids\" indicating the total kids and teens in the home."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine feature kids and teens into one category Kids\n",
        "\n",
        "# Assuming 'data2' is your DataFrame\n",
        "data2['Kids'] = data2['Kidhome'] + data2['Teenhome']\n",
        "# Now you can drop 'Kidhome' and 'Teenhome' if needed\n",
        "data2 = data2.drop(['Kidhome', 'Teenhome'], axis=1)\n"
      ],
      "metadata": {
        "id": "XJcl5iUyth_w"
      },
      "id": "XJcl5iUyth_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "BW9JzmgMtrxE"
      },
      "id": "BW9JzmgMtrxE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b07938c1",
      "metadata": {
        "id": "b07938c1"
      },
      "source": [
        "### **Family Size**\n",
        "* Let's create a new variable called 'Family Size' to find out how many members each family has.\n",
        "* For this, we need to have a look at the Marital_Status variable, and see what are the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44b6c0b",
      "metadata": {
        "id": "b44b6c0b"
      },
      "outputs": [],
      "source": [
        "# Check the unique categories in Marial_Status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check unique categories in Marital_Status\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'data2'\n",
        "# Replace 'data2' with your actual DataFrame name if it's different\n",
        "\n",
        "unique_marital_statuses = data2['Marital_Status'].unique()\n",
        "unique_marital_statuses\n"
      ],
      "metadata": {
        "id": "ZK1S72UcvsRD"
      },
      "id": "ZK1S72UcvsRD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "56bfde90",
      "metadata": {
        "id": "56bfde90"
      },
      "source": [
        "* We can combine the sub-categories Single, Divorced, Widow as \"Single\" and we can combine the sub-categories Married and Together as \"Relationship\"\n",
        "* Then we can create a new variable called \"Status\" and assign values 1 and 2 to categories Single and Relationship, respectively.\n",
        "* Then, we can use the Kids (calculated above) and the Status column to find the family size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c292479",
      "metadata": {
        "id": "6c292479"
      },
      "outputs": [],
      "source": [
        "# Replace \"Married\" and \"Together\" with \"Relationship\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine married and together with relationship\n",
        "\n",
        "# Assuming your data is in a DataFrame called 'data2'\n",
        "# Replace 'data2' with your actual DataFrame name if it's different\n",
        "\n",
        "# Combine marital statuses\n",
        "data2['Marital_Status'] = data2['Marital_Status'].replace(['Married', 'Together'], 'Relationship')\n",
        "data2['Marital_Status'] = data2['Marital_Status'].replace(['Single', 'Divorced', 'Widow', 'Alone', 'Absurd', 'YOLO'], 'Single')\n",
        "\n",
        "# Create the 'Status' column based on the combined categories\n",
        "data2['Status'] = data2['Marital_Status'].map({'Relationship': 2, 'Single': 1})\n",
        "\n",
        "#Calculate Family Size\n",
        "data2['Family_Size'] = data2['Kids'] + data2['Status']\n",
        "\n",
        "# Now you can drop the temporary 'Status' column if you don't need it anymore.\n",
        "data2 = data2.drop('Status', axis = 1)\n",
        "\n",
        "data2.head()\n"
      ],
      "metadata": {
        "id": "3z2HNaN3v7Hh"
      },
      "id": "3z2HNaN3v7Hh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "58cc3f74",
      "metadata": {
        "id": "58cc3f74"
      },
      "source": [
        "### **Expenses**\n",
        "* Let's create a new feature called \"Expenses\", indicating the total amount spent by the customers in various products over the span of two years."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new feature\n",
        "# Add the amount spent on each of product 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'\n",
        "\n",
        "# Assuming 'data2' is the DataFrame\n",
        "data2['Expenses'] = data2['MntWines'] + data2['MntFruits'] + data2['MntMeatProducts'] + data2['MntFishProducts'] + data2['MntSweetProducts'] + data2['MntGoldProds']\n",
        "data2.head()\n"
      ],
      "metadata": {
        "id": "R9_24QYoxDme"
      },
      "id": "R9_24QYoxDme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "18e22798",
      "metadata": {
        "id": "18e22798"
      },
      "source": [
        "### **Total Purchases**\n",
        "* Let's create a new feature called \"NumTotalPurchases\", indicating the total number of products purchased by the customers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a new feature called \"NumTotalPurchases\", indicating the total number of products purchased by the customers.\n",
        "\n",
        "data2[\"NumTotalPurchases\"] = data2['NumDealsPurchases'] + data2['NumWebPurchases'] + data2['NumCatalogPurchases'] + data2['NumStorePurchases']\n"
      ],
      "metadata": {
        "id": "SXYPLwC6xSg7"
      },
      "id": "SXYPLwC6xSg7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "zy_5rWmdxZdq"
      },
      "id": "zy_5rWmdxZdq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type Dt_Customer\n",
        "\n",
        "print(type(data2['Dt_Customer'][0]))\n"
      ],
      "metadata": {
        "id": "UqVWKcoCxkty"
      },
      "id": "UqVWKcoCxkty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2['Dt_Customer'] = pd.to_datetime(data2['Dt_Customer'], format='%d-%m-%Y')"
      ],
      "metadata": {
        "id": "CQeFp1MJ5KFR"
      },
      "id": "CQeFp1MJ5KFR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "qu8IN8aF33HQ"
      },
      "id": "qu8IN8aF33HQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "28ff39bc",
      "metadata": {
        "id": "28ff39bc"
      },
      "source": [
        "### **Engaged in Days**\n",
        "* Let's create a new feature called \"Engaged in days\", indicating how long the customer has been with the company."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new column \"EngagedDays\" indicates how long customer has been with company\n",
        "\n",
        "import pandas as pd\n",
        "# Convert 'Dt_Customer' to datetime objects\n",
        "data2['Dt_Customer'] = pd.to_datetime(data2['Dt_Customer'])\n",
        "\n",
        "# Calculate the number of days since the customer's first purchase\n",
        "data2['EngagedDays'] = (pd.to_datetime('2016-12-09') - data2['Dt_Customer']).dt.days\n",
        "\n",
        "data2.head()\n"
      ],
      "metadata": {
        "id": "WdkK6w6a4H4V"
      },
      "id": "WdkK6w6a4H4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41ba9161",
      "metadata": {
        "id": "41ba9161"
      },
      "source": [
        "**Let's check the max and min of the date.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max and min engaged dates\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and 'Dt_Customer' is the date column.\n",
        "# Replace 'data2' with your actual DataFrame name if necessary.\n",
        "\n",
        "print(f\"Maximum engagement date: {data2['Dt_Customer'].max()}\")\n",
        "print(f\"Minimum engagement date: {data2['Dt_Customer'].min()}\")\n"
      ],
      "metadata": {
        "id": "Rgy89p0n5S9I"
      },
      "id": "Rgy89p0n5S9I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0a8eed51",
      "metadata": {
        "id": "0a8eed51"
      },
      "source": [
        "**Think About It:**\n",
        "- From the above output from the max function, we observed that the last customer enrollment date is December 6th, 2014. Can we extract the number of days a customer has been with the company using some date as the threshold? Can January 1st, 2015 be that threshold?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc01a9c",
      "metadata": {
        "id": "dfc01a9c"
      },
      "outputs": [],
      "source": [
        " # Assigning date to the day variable\n",
        "data2[\"day\"] = \"01-01-2015\"\n",
        "\n",
        "# Converting the variable day to Python datetime object\n",
        "data2[\"day\"] = pd.to_datetime(data2.day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bd9f4a",
      "metadata": {
        "id": "f3bd9f4a"
      },
      "outputs": [],
      "source": [
        "data2[\"Engaged_in_days\"] = (data2[\"day\"] - data2[\"Dt_Customer\"]).dt.days"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "1SVDqxar2CvT"
      },
      "id": "1SVDqxar2CvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.describe()"
      ],
      "metadata": {
        "id": "mqN1RCnS2tkS"
      },
      "id": "mqN1RCnS2tkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "05feb89e",
      "metadata": {
        "id": "05feb89e"
      },
      "source": [
        "### **TotalAcceptedCmp**\n",
        "* Let's create a new feature called \"TotalAcceptedCmp\" that shows how many offers customers have accepted."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a new feature called \"TotalAcceptedCmp\" that shows how many offers customers have accepted.\n",
        "\n",
        "# Assuming 'data2' is your DataFrame\n",
        "data2['TotalAcceptedCmp'] = data2['AcceptedCmp1'] + data2['AcceptedCmp2'] + data2['AcceptedCmp3'] + data2['AcceptedCmp4'] + data2['AcceptedCmp5'] + data2['Response']\n",
        "\n",
        "data2.head()\n"
      ],
      "metadata": {
        "id": "VkplGoun1TJH"
      },
      "id": "VkplGoun1TJH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2193db3",
      "metadata": {
        "id": "d2193db3"
      },
      "source": [
        "### **AmountPerPurchase**\n",
        "* Let's create a new feature called \"AmountPerPurchase\" indicating the amount spent per purchase."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the \"Expenses\" by \"NumTotalPurchases\" to create the new feature AmountPerPurchase\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "data2['AmountPerPurchase'] = data2['Expenses'] / data2['NumTotalPurchases']\n"
      ],
      "metadata": {
        "id": "9ZHpi7A750W9"
      },
      "id": "9ZHpi7A750W9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "52b9ee71",
      "metadata": {
        "id": "52b9ee71"
      },
      "source": [
        "**Now, let's check the maximum value of the AmountPerPurchase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3583b89",
      "metadata": {
        "id": "f3583b89"
      },
      "outputs": [],
      "source": [
        "# Check the max value\n",
        "# Hint: Use max() function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: min and max amountperpurchase\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "print(f\"Maximum AmountPerPurchase: {data2['AmountPerPurchase'].max()}\")\n",
        "print(f\"Minimum AmountPerPurchase: {data2['AmountPerPurchase'].min()}\")\n"
      ],
      "metadata": {
        "id": "ArKzY-OV571C"
      },
      "id": "ArKzY-OV571C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3cf0f01b",
      "metadata": {
        "id": "3cf0f01b"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- Is the maximum value in the above output valid? What could be the potential reason for such output?\n",
        "- How many such values are there? Can we drop such observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c4407c",
      "metadata": {
        "id": "d3c4407c"
      },
      "outputs": [],
      "source": [
        "# Find how many observations have NumTotalPurchases equal to 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amount of numtotalpurchases is 0\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "print(f\"Number of purchases with 0 total purchases: {data2[data2['NumTotalPurchases'] == 0].shape[0]}\")\n"
      ],
      "metadata": {
        "id": "V7XDtafp6HVn"
      },
      "id": "V7XDtafp6HVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b4529e",
      "metadata": {
        "id": "78b4529e"
      },
      "outputs": [],
      "source": [
        "# Drop the observations with NumTotalPurchases equal to 0, using their indices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop numtotalpurchases equal to o using indices\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "indices_to_drop = data2[data2['NumTotalPurchases'] == 0].index\n",
        "data2.drop(indices_to_drop, inplace=True)\n",
        "\n",
        "# Verify the change (optional)\n",
        "print(f\"Number of purchases with 0 total purchases after dropping: {data2[data2['NumTotalPurchases'] == 0].shape[0]}\")\n"
      ],
      "metadata": {
        "id": "AeqBCGrb6Saw"
      },
      "id": "AeqBCGrb6Saw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ffba995d",
      "metadata": {
        "id": "ffba995d"
      },
      "source": [
        "**Now, let's check the distribution of values in AmountPerPurchase column.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc682e26",
      "metadata": {
        "id": "fc682e26"
      },
      "outputs": [],
      "source": [
        "# Check the summary statistics of the AmountPerPurchase variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: summary statistics of amountperpurchase\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "print(data2['AmountPerPurchase'].describe())\n"
      ],
      "metadata": {
        "id": "5k-N07LR6e0M"
      },
      "id": "5k-N07LR6e0M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5de5de5",
      "metadata": {
        "id": "c5de5de5"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for the AmountPerPurchas variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: histogram of amountperpurchase\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data2['AmountPerPurchase'], bins=30)  # Adjust the number of bins as needed\n",
        "plt.title('Distribution of Amount Per Purchase')\n",
        "plt.xlabel('Amount Per Purchase')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CgftW7JO6mfE"
      },
      "id": "CgftW7JO6mfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "40e75013",
      "metadata": {
        "id": "40e75013"
      },
      "source": [
        "#### **Observations and Insights: **\n",
        "Overall Purchase Behavior:\n",
        "Total Spending: You can calculate the total spending of each customer by summing up the amounts spent on different product categories (MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds). This will give you an overall idea of customer spending habits.\n",
        "\n",
        "Purchase Channels: Analyze the number of purchases made through different channels (NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases) to understand customer preferences for purchasing methods.\n",
        "\n",
        "Product Preferences: Examine the amounts spent on individual product categories to identify popular products and potential customer segments based on product preferences.\n",
        "\n",
        "Observations and Insights:\n",
        "High-Value Customers: Identify customers with high total spending and frequent purchases. These are your most valuable customers, and you can tailor marketing strategies to retain them.\n",
        "Channel Preferences: Determine the preferred purchase channels for different customer segments. This information can help you optimize your marketing efforts and channel strategies.\n",
        "Product Bundling Opportunities: If you find correlations between purchases of different product categories, consider offering product bundles or promotions to encourage cross-selling.\n",
        "Seasonal Trends: Explore if there are any seasonal trends in purchase behavior. This can help you plan targeted campaigns and inventory management.\n",
        "Campaign Effectiveness: Analyze the impact of previous marketing campaigns (AcceptedCmp1 to AcceptedCmp5, Response) on customer purchase behavior. This can help you assess campaign effectiveness and refine future campaigns."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Lfoq1Tb7FL4"
      },
      "id": "2Lfoq1Tb7FL4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGlhHGvl7FH1"
      },
      "id": "FGlhHGvl7FH1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "49975b70",
      "metadata": {
        "id": "49975b70"
      },
      "source": [
        "### **Imputing Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741d5135",
      "metadata": {
        "id": "741d5135"
      },
      "outputs": [],
      "source": [
        "# Impute the missing values for the Income variable with the median"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find missing values in income\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "# Fill missing 'Income' values with the median income.\n",
        "data2['Income'].fillna(data2['Income'].median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "3NZJec0E7WD9"
      },
      "id": "3NZJec0E7WD9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cba3899e",
      "metadata": {
        "id": "cba3899e"
      },
      "source": [
        "**Now that we are done with data preprocessing, let's visualize new features against the new income variable we have after imputing missing values.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e7c103",
      "metadata": {
        "id": "41e7c103"
      },
      "source": [
        "### **Income Vs Expenses**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a scatterplot using data2 to show expenses on y and income on x\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data2' is your DataFrame and it's already loaded.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Income', y='Expenses', data=data2)\n",
        "plt.title('Income vs Expenses')\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Expenses')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JwjfSJUp8fHD"
      },
      "id": "JwjfSJUp8fHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e09f35db",
      "metadata": {
        "id": "e09f35db"
      },
      "source": [
        "#### **Observations and Insights: _____**\n",
        "Upward Trend: The most prominent observation would be a general upward trend in the scatterplot. This indicates that as income increases, expenses also tend to increase. This is a common pattern observed in financial data.\n",
        "\n",
        "Variability: While there is a general upward trend, you'll notice that the points are not perfectly aligned along a straight line. This indicates variability in spending habits among individuals with similar incomes. Some people might spend more, while others spend less, even with the same income level.\n",
        "\n",
        "Potential Outliers: Look for points that are significantly far from the main cluster of points. These outliers could represent individuals with unusual spending patterns, data entry errors, or exceptional circumstances. It's worth investigating these outliers further to understand the reasons behind their unusual behavior.\n",
        "\n",
        "Clustering: You might observe clusters of points forming in specific regions of the scatterplot. This could indicate different spending behaviors within certain income groups. For example, there might be a cluster of points representing individuals with lower incomes who spend a larger proportion of their income on essential needs, while another cluster might represent individuals with higher incomes who spend more on discretionary items.\n",
        "\n",
        "Income-Expense Relationship: Observe how the steepness of the upward trend changes across different income levels. This provides insights into the relationship between income and expenses. For example, if the trend becomes steeper at higher income levels, it suggests that higher-income individuals tend to spend a larger proportion of their income compared to those with lower incomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72064877",
      "metadata": {
        "id": "72064877"
      },
      "source": [
        "### **Family Size Vs Income**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d4b877",
      "metadata": {
        "id": "e6d4b877"
      },
      "outputs": [],
      "source": [
        "# Plot the bar plot for Family Size on X-axis and Income on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a bar plot for Family size on X axis and Income on the y for data 2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Family_Size', y='Income', data=data2)\n",
        "plt.title('Family Size vs Income')\n",
        "plt.xlabel('Family Size')\n",
        "plt.ylabel('Income')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9veHY1419HrX"
      },
      "id": "9veHY1419HrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98dd0473",
      "metadata": {
        "id": "98dd0473"
      },
      "source": [
        "#### **Observations and Insights: _____**\n",
        "\n",
        "Income and Family Size: There is a noticeable relationship between family size and average income. This relationship is typically inverse, meaning that as family size increases, the average income tends to decrease.\n",
        "\n",
        "Highest Income: Households with smaller family sizes (e.g., 1 or 2 members) generally have the highest average income. This could be due to having fewer dependents and therefore lower expenses.\n",
        "\n",
        "Decreasing Income: As the family size increases (e.g., 3, 4, or more members), the average income tends to decrease. This could reflect the increased financial burden associated with supporting more dependents.\n",
        "\n",
        "Variability: You'll likely observe some variability in income within each family size category. This indicates that income is influenced by other factors besides family size, such as education, occupation, and location.\n",
        "\n",
        "Potential Outliers: Look for any bars that are significantly higher or lower than the general trend. These could represent outliers – families with unusually high or low incomes for their family size. Further investigation might be needed to understand these cases.\n",
        "\n",
        "These observations provide insights into the relationship between family size and income. While larger families may face greater financial challenges, it's important to remember that individual circumstances and other factors also play a significant role in income levels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "25ZFtETwQuTB"
      },
      "id": "25ZFtETwQuTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: list all coloumns in data2\n",
        "\n",
        "data2.columns.tolist()\n"
      ],
      "metadata": {
        "id": "3s5ZYBz7Rnc1"
      },
      "id": "3s5ZYBz7Rnc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "NM-t3IzKRvmw"
      },
      "id": "NM-t3IzKRvmw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop day and Engaged_days\n",
        "\n",
        "# Assuming 'data2' is your DataFrame.\n",
        "data2 = data2.drop(['day', 'Engaged_in_days'], axis=1)\n"
      ],
      "metadata": {
        "id": "_DQYS4HcTcQw"
      },
      "id": "_DQYS4HcTcQw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "Cn4GFokjTgMW"
      },
      "id": "Cn4GFokjTgMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98fded39",
      "metadata": {
        "id": "98fded39"
      },
      "source": [
        "## **Important Insights from EDA and Data Preprocessing**\n",
        "\n",
        "What are the the most important observations and insights from the data based on the EDA and Data Preprocessing performed?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac2abaf",
      "metadata": {
        "id": "fac2abaf"
      },
      "source": [
        "## Preparing Data for Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f7bad9",
      "metadata": {
        "id": "37f7bad9"
      },
      "source": [
        "### Dropping columns that we will not use for segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec456758",
      "metadata": {
        "id": "ec456758"
      },
      "source": [
        "The decision about which variables to use for clustering is a critically important decision that will have a big impact on the clustering solution. So we need to think carefully about the variables we will choose for clustering. Clearly, this is a step where a lot of contextual knowledge, creativity, and experimentation/iterations are needed.\n",
        "\n",
        "Moreover, we often use only a few of the data attributes for segmentation (the segmentation attributes) and use some of the remaining ones (the profiling attributes) only to profile the clusters. For example, in market research and market segmentation, we can use behavioral data for segmentation (to segment the customers based on their behavior like amount spent, units bought, etc.), and then use both demographic as well as behavioral data for profiling the segments found.\n",
        "\n",
        "Here, we will use the behavioral attributes for segmentation and drop the demographic attributes like Income, Age, and Family_Size. In addition to this, we need to drop some other columns which are mentioned below.\n",
        "\n",
        "* `Dt_Customer`: We have created the `Engaged_in_days` variable using the Dt_Customer variable. Hence, we can drop this variable as it will not help with segmentation.\n",
        "* `Complain`: About 95% of the customers didn't complain and have the same value for this column. This variable will not have a major impact on segmentation. Hence, we can drop this variable.\n",
        "* `day`:  We have created the `Engaged_in_days` variable using the 'day' variable. Hence, we can drop this variable as it will not help with segmentation.\n",
        "* `Status`: This column was created just to get the `Family_Size` variable that contains the information about the Status. Hence, we can drop this variable.\n",
        "* We also need to drop categorical variables like `Education` and `Marital_Status`, `Kids`, `Kidhome`, and `Teenhome` as distance-based algorithms cannot use the default distance like Euclidean to find the distance between categorical and numerical variables.\n",
        "* We can also drop categorical variables like `AcceptedCmp1`, `AcceptedCmp2`, `AcceptedCmp3`, `AcceptedCmp4`, `AcceptedCmp5`, and `Response` for which we have create the variable `TotalAcceptedCmp` which is the aggregate of all these variables."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create data3 copy of data2\n",
        "\n",
        "data3 = data2.copy()\n"
      ],
      "metadata": {
        "id": "71TXrC279sV1"
      },
      "id": "71TXrC279sV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3.head()"
      ],
      "metadata": {
        "id": "qe-9ynr0_AOy"
      },
      "id": "qe-9ynr0_AOy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ce76d3",
      "metadata": {
        "id": "f0ce76d3"
      },
      "outputs": [],
      "source": [
        "# Dropping all the irrelevant columns and storing in data_model\n",
        "data_model = data3.drop(\n",
        "    columns=[\n",
        "        \"Dt_Customer\",\n",
        "        \"Complain\",\n",
        "        \"Response\",\n",
        "        \"AcceptedCmp1\",\n",
        "        \"AcceptedCmp2\",\n",
        "        \"AcceptedCmp3\",\n",
        "        \"AcceptedCmp4\",\n",
        "        \"AcceptedCmp5\",\n",
        "        \"Marital_Status\",\n",
        "        'Education', 'Income','Age', 'Family_Size', 'Kids', 'EngagedDays', 'TotalAcceptedCmp'\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdGFtbpaAk8U"
      },
      "id": "BdGFtbpaAk8U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad26b8d",
      "metadata": {
        "id": "fad26b8d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Check the shape of new data\n",
        "data_model.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de55f99b",
      "metadata": {
        "id": "de55f99b"
      },
      "outputs": [],
      "source": [
        "# Check first five rows of new data\n",
        "data_model.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: list column names\n",
        "\n",
        "print(data_model.columns.tolist())\n"
      ],
      "metadata": {
        "id": "7bctVZxgA8IR"
      },
      "id": "7bctVZxgA8IR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7e6baf94",
      "metadata": {
        "id": "7e6baf94"
      },
      "source": [
        "**Let's plot the correlation plot after we've removed the irrelevant variables.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: correlation plot for data_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data_model' is your DataFrame\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(data_model.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Plot of Data Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7AoYG-IhBB2O"
      },
      "id": "7AoYG-IhBB2O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7a113f41",
      "metadata": {
        "id": "7a113f41"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f86dd84",
      "metadata": {
        "id": "7f86dd84"
      },
      "source": [
        "### Scaling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754d65a4",
      "metadata": {
        "id": "754d65a4"
      },
      "source": [
        "**What is feature scaling?**\n",
        "\n",
        "Feature scaling is a class of statistical techniques that, as the name implies, scales the features of our data so that they all have a similar range. You'll understand better if we look at an example:\n",
        "\n",
        "If you have multiple independent variables like Age, Income, and Amount related variables, with their range as (18–100 Years), (25K–75K), and (100–200), respectively, feature scaling would help them all to be in the same range.\n",
        "\n",
        "**Why feature scaling is important in Unsupervised Learning?**\n",
        "\n",
        "Feature scaling is especially relevant in machine learning models that compute some sort of distance metric as we do in most clustering algorithms, for example, K-Means.\n",
        "\n",
        "So, scaling should be done to avoid the problem of one feature dominating over others because the unsupervised learning algorithm uses distance to find the similarity between data points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f91b734",
      "metadata": {
        "id": "8f91b734"
      },
      "source": [
        "**Let's scale the data**\n",
        "\n",
        "**Standard Scaler**: StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.\n",
        "\n",
        "\n",
        "\n",
        "1. Data standardization is the process of rescaling the attributes so that they have a mean of 0 and a variance of 1.\n",
        "2. The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values.\n",
        "3. In sklearn.preprocessing.StandardScaler(), centering and scaling happen independently on each feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Applying standard scaler on new data\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "import numpy as np\n",
        "# Assuming 'data_model' is your DataFrame and it's already loaded.\n",
        "# We need to select numerical features for scaling.\n",
        "numerical_features = data_model.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Apply StandardScaler to the numerical features\n",
        "scaler = StandardScaler()\n",
        "data_model[numerical_features] = scaler.fit_transform(data_model[numerical_features])\n",
        "\n",
        "# Now 'data_model' contains the scaled numerical features.\n"
      ],
      "metadata": {
        "id": "u0WHPfefEqVh"
      },
      "id": "u0WHPfefEqVh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d39752f",
      "metadata": {
        "id": "2d39752f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Applying standard scaler on new data\n",
        "#scaler = StandardScaler()                                                  # Initialize the Standard Scaler\n",
        "\n",
        "#df_scaled = scaler.fit                                       # fit_transform the scaler function on new data\n",
        "\n",
        "#df_scaled = pd.DataFrame(df_scaled, columns=data_model.columns)      # Converting the embeddings to a dataframe\n",
        "\n",
        "#df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1a656f",
      "metadata": {
        "id": "9c1a656f"
      },
      "source": [
        "## **Applying T-SNE and PCA to the data to visualize the data distributed in 2 dimensions**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0324acb",
      "metadata": {
        "id": "f0324acb"
      },
      "source": [
        "### **Applying T-SNE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Fitting T-SNE with number of components equal to 2 to visualize how data is distributed\n",
        "# tsne = _____________        # Initializing T-SNE with number of component equal to 2, random_state=1, and perplexity=35\n",
        "# data_air_pol_tsne = ___________                            # fit_transform T-SNE on new data\n",
        "# data_air_pol_tsne = pd.DataFrame(data_air_pol_tsne, columns=[0, 1])           # Converting the embeddings to a dataframe\n",
        "# plt.figure(figsize=(7, 7))                                                    # Scatter plot for two components\n",
        "# sns.scatterplot(x=0, y=1, data=data_air_pol_tsne)                             # Plotting T-SNE\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "tsne = TSNE(n_components=2, random_state=1, perplexity=35)        # Initializing T-SNE with number of component equal to 2, random_state=1, and perplexity=35\n",
        "data_air_pol_tsne = tsne.fit_transform(data_model)                            # fit_transform T-SNE on new data\n",
        "data_air_pol_tsne = pd.DataFrame(data_air_pol_tsne, columns=[0, 1])           # Converting the embeddings to a dataframe\n",
        "plt.figure(figsize=(7, 7))                                                    # Scatter plot for two components\n",
        "sns.scatterplot(x=0, y=1, data=data_air_pol_tsne)                             # Plotting T-SNE\n"
      ],
      "metadata": {
        "id": "ltZBYK5EFBeq"
      },
      "id": "ltZBYK5EFBeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c3539d",
      "metadata": {
        "id": "c2c3539d"
      },
      "outputs": [],
      "source": [
        "# Fitting T-SNE with number of components equal to 2 to visualize how data is distributed\n",
        "\n",
        "#tsne = _____________        # Initializing T-SNE with number of component equal to 2, random_state=1, and perplexity=35\n",
        "\n",
        "#data_air_pol_tsne = ___________                            # fit_transform T-SNE on new data\n",
        "\n",
        "#data_air_pol_tsne = pd.DataFrame(data_air_pol_tsne, columns=[0, 1])           # Converting the embeddings to a dataframe\n",
        "\n",
        "#plt.figure(figsize=(7, 7))                                                    # Scatter plot for two components\n",
        "\n",
        "#sns.scatterplot(x=0, y=1, data=data_air_pol_tsne)                             # Plotting T-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5f30dc",
      "metadata": {
        "id": "2a5f30dc"
      },
      "source": [
        "**Observation and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f715ff",
      "metadata": {
        "id": "c4f715ff"
      },
      "source": [
        "### **Applying PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f028e30",
      "metadata": {
        "id": "8f028e30"
      },
      "source": [
        "**Think about it:**\n",
        "- Should we apply clustering algorithms on the current data or should we apply PCA on the data before applying clustering algorithms? How would this help?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132914a3",
      "metadata": {
        "id": "132914a3"
      },
      "source": [
        "When the variables used in clustering are highly correlated, it causes multicollinearity, which affects the clustering method and results in poor cluster profiling (or biased toward a few variables). PCA can be used to reduce the multicollinearity between the variables."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Defining the number of principal components to generate\n",
        "# n = data_model.shape[1]                                        # Storing the number of variables in the data\n",
        "# pca = _________________                                        # Initialize PCA with n_components = n and random_state=1\n",
        "# data_pca = pd.DataFrame(pca.____________)                      # fit_transform PCA on the scaled data\n",
        "# # The percentage of variance explained by each principal component is stored\n",
        "# exp_var = pca.explained_variance_ratio_\n",
        "\n",
        "import pandas as pd\n",
        "# Defining the number of principal components to generate\n",
        "n = data_model.shape[1]                                        # Storing the number of variables in the data\n",
        "pca = PCA(n_components=n, random_state=1)                                        # Initialize PCA with n_components = n and random_state=1\n",
        "data_pca = pd.DataFrame(pca.fit_transform(data_model))                      # fit_transform PCA on the scaled data\n",
        "# The percentage of variance explained by each principal component is stored\n",
        "exp_var = pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "3tgVJ4RSFXNe"
      },
      "id": "3tgVJ4RSFXNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482ba0e6",
      "metadata": {
        "id": "482ba0e6"
      },
      "outputs": [],
      "source": [
        "# Defining the number of principal components to generate\n",
        "#n = data_model.shape[1]                                        # Storing the number of variables in the data\n",
        "\n",
        "#pca = _________________                                        # Initialize PCA with n_components = n and random_state=1\n",
        "\n",
        "#data_pca = pd.DataFrame(pca.____________)                      # fit_transform PCA on the scaled data\n",
        "\n",
        "# The percentage of variance explained by each principal component is stored\n",
        "#exp_var = pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0305f9e8",
      "metadata": {
        "id": "0305f9e8"
      },
      "source": [
        "**Let's plot the first two components and see how the data points are distributed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f408dbed",
      "metadata": {
        "id": "f408dbed"
      },
      "outputs": [],
      "source": [
        "# Scatter plot for two components using the dataframe data_pca\n",
        "plt.figure(figsize=(7, 7))\n",
        "sns.scatterplot(x=0, y=1, data=data_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5856be29",
      "metadata": {
        "id": "5856be29"
      },
      "source": [
        "**Let's apply clustering algorithms on the data generated after applying PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55dd08b",
      "metadata": {
        "id": "f55dd08b"
      },
      "source": [
        "## **K-Means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f055a8fe",
      "metadata": {
        "id": "f055a8fe"
      },
      "outputs": [],
      "source": [
        "distortions = []                                                  # Create an empty list\n",
        "\n",
        "K = range(2, 10)                                                  # Setting the K range from 2 to 10\n",
        "\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k,random_state=4)              # Initialize K-Means\n",
        "    kmeanModel.fit(data_pca)                                      # Fit K-Means on the data\n",
        "    distortions.append(kmeanModel.inertia_)                       # Append distortion values to the empty list created above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a849a4",
      "metadata": {
        "id": "b2a849a4"
      },
      "outputs": [],
      "source": [
        "# Plotting the elbow plot\n",
        "plt.figure(figsize=(16, 8))                                            # Setting the plot size\n",
        "\n",
        "plt.plot(K, distortions, \"bx-\")                                        # Plotting the K on X-axis and distortions on y-axis\n",
        "\n",
        "plt.xlabel(\"k\")                                                        # Title of x-axis\n",
        "\n",
        "plt.ylabel(\"Distortion\")                                               # Title of y-axis\n",
        "\n",
        "plt.title(\"The Elbow Method showing the optimal k\")                    # Title of the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3857aa08",
      "metadata": {
        "id": "3857aa08"
      },
      "source": [
        "**In the above plot, the elbow is seen for K=3 and K=5 as there is some drop in distortion at K=3 and K=5.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APJ-yYgQaY2I"
      },
      "id": "APJ-yYgQaY2I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "edfa1df7",
      "metadata": {
        "id": "edfa1df7"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- How do we determine the optimal K value when the elbows are observed at 2 or more K values from the elbow curve?\n",
        "- Which metric can be used to determine the final K value?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afcc3d32",
      "metadata": {
        "id": "afcc3d32"
      },
      "source": [
        "**We can use the silhouette score as a metric for different K values to make a better decision about picking the number of clusters(K).**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e55428",
      "metadata": {
        "id": "f9e55428"
      },
      "source": [
        "### **What is the silhouette score?**\n",
        "\n",
        "Silhouette score is one of the methods for evaluating the quality of clusters created using clustering algorithms such as K-Means. The silhouette score is a measure of how similar an object is to its cluster (cohesion) compared to other clusters (separation). Silhouette score has a range of [-1, 1].\n",
        "\n",
        "* Silhouette coefficients near +1 indicate that the clusters are dense and well separated, which is good.\n",
        "* Silhouette score near -1 indicates that those samples might have been assigned to the wrong cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0e5c3e",
      "metadata": {
        "id": "cb0e5c3e"
      },
      "source": [
        "**Finding silhouette score for each value of K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3f65f3",
      "metadata": {
        "id": "6f3f65f3"
      },
      "outputs": [],
      "source": [
        "sil_score = []                                                             # Creating empty list\n",
        "cluster_list = range(3, 7)                                                 # Creating a range from 3 to 7\n",
        "for n_clusters in cluster_list:\n",
        "\n",
        "    # Initialize K-Means with number of clusters equal to n_clusters and random_state=1\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    # Fit and predict on the pca data\n",
        "    preds = clusterer.fit_predict(data_pca)\n",
        "\n",
        "    # Calculate silhouette score - Hint: Use silhouette_score() function\n",
        "    score = silhouette_score(data_pca, preds)\n",
        "\n",
        "    # Append silhouette score to empty list created above\n",
        "    sil_score.append(score)\n",
        "\n",
        "    # Print the silhouette score\n",
        "    print( \"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ccc59a8",
      "metadata": {
        "id": "3ccc59a8"
      },
      "source": [
        "**From the above silhouette scores, 3 appears to be a good value of K. So, let's build K-Means using K=3.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9f50a7",
      "metadata": {
        "id": "da9f50a7"
      },
      "source": [
        "### **Applying K-Means on data_pca**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed152db9",
      "metadata": {
        "id": "ed152db9"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=1)  # Initialize the K-Means algorithm with 3 clusters and random_state=1\n",
        "kmeans.fit(data_pca)                                     # Fitting on the data_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4823f6",
      "metadata": {
        "id": "ba4823f6"
      },
      "outputs": [],
      "source": [
        "data_pca[\"K_means_segments_3\"] = kmeans.labels_                    # Adding K-Means cluster labels to the data_pca data\n",
        "\n",
        "data_model[\"K_means_segments_3\"] = kmeans.labels_                        # Adding K-Means cluster labels to the whole data\n",
        "\n",
        "data_model[\"K_means_segments_3\"] = kmeans.labels_                  # Adding K-Means cluster labels to data_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2[\"k_means_segments_3\"] = kmeans.labels_"
      ],
      "metadata": {
        "id": "AX19Ie26WdGQ"
      },
      "id": "AX19Ie26WdGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "axn2JvPOWhuk"
      },
      "id": "axn2JvPOWhuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b79c56",
      "metadata": {
        "id": "95b79c56"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n",
        "data_model[\"K_means_segments_3\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39115d43",
      "metadata": {
        "id": "39115d43"
      },
      "source": [
        "**Let's visualize the clusters using PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3847ec74",
      "metadata": {
        "id": "3847ec74"
      },
      "outputs": [],
      "source": [
        "# Function to visualize PCA data with clusters formed\n",
        "def PCA_PLOT(X, Y, PCA, cluster):\n",
        "    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9f9cd8",
      "metadata": {
        "id": "af9f9cd8"
      },
      "outputs": [],
      "source": [
        "PCA_PLOT(0, 1, data_pca, \"K_means_segments_3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7058a4a7",
      "metadata": {
        "id": "7058a4a7"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "Observations:\n",
        "\n",
        "Cluster 0 (High Income, High Spenders): This cluster represents customers with the highest average income and the highest spending across most product categories (wines, meat, gold products, etc.). They also tend to be older and have fewer children. They are more likely to have accepted campaigns and respond positively to marketing efforts.\n",
        "Cluster 1 (Moderate Income, Moderate Spenders): This cluster has a moderate income and spending behavior. They are more likely to have children and teenagers at home. Their engagement with campaigns and response rate is moderate.\n",
        "Cluster 2 (Low Income, Low Spenders): This cluster consists of customers with the lowest average income and the lowest spending on products. They are more likely to be younger and have fewer web visits. Their acceptance rate for campaigns and response rate is also relatively lower.\n",
        "Insights:\n",
        "\n",
        "Targeting High-Value Customers: Cluster 0 represents the most valuable customer segment due to their high income and spending. Marketing efforts should prioritize personalized campaigns and premium offerings to retain these customers.\n",
        "Potential for Growth: Cluster 1 represents customers with moderate income and spending, suggesting potential for increased engagement and purchasing behavior. Targeted campaigns focused on value and family-oriented products could encourage higher spending within this group.\n",
        "Understanding Price Sensitivity: Cluster 2 demonstrates price sensitivity, making them less likely to respond to premium or high-priced campaigns. Marketing strategies for this group should emphasize discounts, promotions, and value-driven products.\n",
        "Tailoring Channels: The cluster profiling reveals differences in channel preferences (e.g., web purchases, store purchases). Marketing campaigns can be optimized by aligning channel strategies with the preferences of each cluster.\n",
        "Life Stage Considerations: Income and spending patterns are influenced by life stages (e.g., family size, age). Marketing messages and product offerings should be tailored to resonate with the needs and preferences of customers in different life stages.\n",
        "Campaign Optimization: The insights from cluster profiling can guide the optimization of marketing campaigns by identifying the most receptive customer segments and tailoring messages accordingly.\n",
        "Product Development: The understanding of customer preferences for different product categories across clusters can inform product development initiatives and optimize assortment strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ddfd387",
      "metadata": {
        "id": "9ddfd387"
      },
      "source": [
        "### **Cluster Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean\n",
        "\n",
        "# Group data by cluster labels and calculate the mean of each variable\n",
        "cluster_means = data2.groupby('k_means_segments_3').mean(numeric_only = True)\n",
        "\n",
        "# Print the cluster-wise means\n",
        "cluster_means\n"
      ],
      "metadata": {
        "id": "GVE5E0qOeV8x"
      },
      "id": "GVE5E0qOeV8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_means.style.highlight_max(color = \"lightgreen\", axis = 0)"
      ],
      "metadata": {
        "id": "REtLeTIBZ9U8"
      },
      "id": "REtLeTIBZ9U8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04f3b350",
      "metadata": {
        "id": "04f3b350"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af8109",
      "metadata": {
        "id": "81af8109"
      },
      "source": [
        "**Let us create a boxplot for each of the variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689b06e6",
      "metadata": {
        "id": "689b06e6"
      },
      "outputs": [],
      "source": [
        "# Columns to use in boxplot\n",
        "col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b312bb5",
      "metadata": {
        "id": "1b312bb5"
      },
      "outputs": [],
      "source": [
        "# Creating boxplot for each of the variables\n",
        "all_col = col_for_box\n",
        "\n",
        "plt.figure(figsize = (30, 50))\n",
        "\n",
        "for i, variable in enumerate(all_col):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "\n",
        "    sns.boxplot (y=data[variable], x=data2.select_dtypes(include=np.number)[\"k_means_segments_3\"], showmeans=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380bb5d4",
      "metadata": {
        "id": "380bb5d4"
      },
      "source": [
        "### **Characteristics of each cluster:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd1a949",
      "metadata": {
        "id": "5fd1a949"
      },
      "source": [
        "**Cluster 0:__________**\n",
        "\n",
        "**Summary for cluster 0:_______________**\n",
        "\n",
        "**Cluster 1:_______________**\n",
        "\n",
        "**Summary for cluster 1:_______________**\n",
        "\n",
        "\n",
        "\n",
        "**Cluster 2:_______________**\n",
        "\n",
        "**Summary for cluster 2:_______________**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f8b3d0",
      "metadata": {
        "id": "08f8b3d0"
      },
      "source": [
        "**Think About It:**\n",
        "- Are the K-Means profiles with K=3 providing any deep insights into customer purchasing behavior or which channels they are using?\n",
        "- What is the next step to get more meaningful insights?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a8e859",
      "metadata": {
        "id": "b5a8e859"
      },
      "source": [
        "We can see from the above profiles that K=3 segments the customers into High, Medium and Low-income customers, and we are not getting deep insights into different types of customers. So, let's try to build K=5 (which has another elbow in the Elbow curve) and see if we can get better cluster profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ed9bba",
      "metadata": {
        "id": "00ed9bba"
      },
      "outputs": [],
      "source": [
        "# Dropping labels we got from K=3 since we will be using PCA data for prediction\n",
        "# Drop K_means_segments_3. Hint: Use axis=1 and inplace=True\n",
        "if \"K_means_segments_3\" in data_pca.columns:\n",
        "    data_pca.drop(\"K_means_segments_3\", axis=1, inplace=True)\n",
        "\n",
        "if \"K_means_segments_3\" in data_model.columns:\n",
        "    data_model.drop(\"K_means_segments_3\", axis=1, inplace=True)\n",
        "\n",
        "if \"K_means_segments_3\" in data2.columns:\n",
        "    data2.drop(\"K_means_segments_3\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79fb95f",
      "metadata": {
        "id": "f79fb95f"
      },
      "source": [
        "**Let's build K-Means using K=5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3548bf89",
      "metadata": {
        "id": "3548bf89"
      },
      "outputs": [],
      "source": [
        "# Fit the K-Means algorithm using number of cluster as 5 and random_state=0 on data_pca\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "kmeans.fit(data_pca)\n",
        "data_pca[\"K_means_segments_5\"] = kmeans.labels_\n",
        "data_model[\"K_means_segments_5\"] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2[\"K_means_segments_5\"] = kmeans.labels_"
      ],
      "metadata": {
        "id": "RCNDz691b4e2"
      },
      "id": "RCNDz691b4e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0d6ff7",
      "metadata": {
        "id": "2e0d6ff7"
      },
      "outputs": [],
      "source": [
        "# Add K-Means cluster labels to data_pca\n",
        "data_pca[\"K_means_segments_5\"] = kmeans.labels_\n",
        "# Add K-Means cluster labels to whole data\n",
        "data_model[\"K_means_segments_5\"] = kmeans.labels_\n",
        "# Add K-Means cluster labels to data_model\n",
        "data_model[\"K_means_segments_5\"] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_means = data2.select_dtypes(include=np.number).groupby('K_means_segments_5').mean()\n",
        "cluster_means"
      ],
      "metadata": {
        "id": "-UbMoKsWnUrH"
      },
      "id": "-UbMoKsWnUrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "498c9bf2",
      "metadata": {
        "id": "498c9bf2"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n",
        "print(data2[\"K_means_segments_5\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2[\"K_means_segments_5\"].value_counts()"
      ],
      "metadata": {
        "id": "DcqQ2WLAdmvy"
      },
      "id": "DcqQ2WLAdmvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7e8177be",
      "metadata": {
        "id": "7e8177be"
      },
      "source": [
        "**Let's visualize the clusters using PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd8acff",
      "metadata": {
        "id": "abd8acff"
      },
      "outputs": [],
      "source": [
        "# Hint: Use PCA_PLOT function created above\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PCA_PLOT(X, Y, PCA, cluster):\n",
        "    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)"
      ],
      "metadata": {
        "id": "r_AvAWDzcJme"
      },
      "id": "r_AvAWDzcJme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_PLOT(0, 1, data_pca, \"K_means_segments_5\")"
      ],
      "metadata": {
        "id": "qLNYwz23b-Si"
      },
      "id": "qLNYwz23b-Si",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f65ce959",
      "metadata": {
        "id": "f65ce959"
      },
      "source": [
        "### **Cluster Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7793cc2",
      "metadata": {
        "id": "f7793cc2"
      },
      "outputs": [],
      "source": [
        "# Take the cluster-wise mean of all the variables. Hint: First groupby 'data' by cluster labels column and then find mean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: group data2 by K_means_segments_5\n",
        "\n",
        "cluster_means = data2.groupby('K_means_segments_5').mean(numeric_only = True)\n",
        "cluster_means\n"
      ],
      "metadata": {
        "id": "6HuxYnyefQOD"
      },
      "id": "6HuxYnyefQOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Take the cluster-wise mean of all the variables.  First groupby 'data' by cluster labels column and then find mean\n",
        "\n",
        "cluster_profile_KMeans_5 = data_model.groupby('K_means_segments_5').mean()\n",
        "cluster_means.style.highlight_max(color = \"yellow\", axis = 0).highlight_min(color=\"orange\", axis = 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "pJWds7zUUOAW"
      },
      "id": "pJWds7zUUOAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_f5DbpQf3qW"
      },
      "id": "i_f5DbpQf3qW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0b2964b3",
      "metadata": {
        "id": "0b2964b3"
      },
      "source": [
        "**Let's plot the boxplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c233433",
      "metadata": {
        "id": "9c233433"
      },
      "outputs": [],
      "source": [
        "# Create boxplot for each of the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']"
      ],
      "metadata": {
        "id": "b9RVIM1kgyT3"
      },
      "id": "b9RVIM1kgyT3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create boxplot for each of the variables\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'data_model' and 'data_pca' are already defined and populated.\n",
        "# Also assuming 'col_for_box' is defined.\n",
        "\n",
        "col_for_box = ['Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "all_col = col_for_box\n",
        "\n",
        "plt.figure(figsize=(30, 50))\n",
        "\n",
        "for i, variable in enumerate(all_col):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    sns.boxplot(y=data2[variable], x=data_pca['K_means_segments_5'], showmeans=True) # Use data_model for the variable values\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0Z69FXf5UkEw"
      },
      "id": "0Z69FXf5UkEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00d533fc",
      "metadata": {
        "id": "00d533fc"
      },
      "source": [
        "### **Characteristics of each cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc42d2d3",
      "metadata": {
        "id": "cc42d2d3"
      },
      "source": [
        "**Cluster 0:__________**\n",
        "\n",
        "**Summary for cluster 0:_______________**\n",
        "\n",
        "**Cluster 1:_______________**\n",
        "\n",
        "**Summary for cluster 1:_______________**\n",
        "\n",
        "\n",
        "**Cluster 2:_______________**\n",
        "\n",
        "**Summary for cluster 2:_______________**\n",
        "\n",
        "**Cluster 3:_______________**\n",
        "\n",
        "**Summary for cluster 3:_______________**\n",
        "\n",
        "**Cluster 4:_______________**\n",
        "\n",
        "**Summary for cluster 4:_______________**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c296e7",
      "metadata": {
        "id": "28c296e7"
      },
      "outputs": [],
      "source": [
        "# Dropping labels we got from K-Means since we will be using PCA data for prediction\n",
        "# Hint: Use axis=1 and inplace=True\n",
        "data_model.drop(\"K_means_segments_5\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"K_means_segments_5\" in data_pca.columns:\n",
        "    data_pca.drop(\"K_means_segments_5\", axis=1, inplace=True)\n",
        "\n",
        "if \"K_means_segments_5\" in data_model.columns:\n",
        "    data_model.drop(\"K_means_segments_5\", axis=1, inplace=True)\n",
        "\n",
        "if \"K_means_segments_5\" in data2.columns:\n",
        "    data2.drop(\"K_means_segments_5\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "-RYPJ_BSlqhQ"
      },
      "id": "-RYPJ_BSlqhQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d943ef79",
      "metadata": {
        "id": "d943ef79"
      },
      "source": [
        "From the above profiles, K=5 provides more interesting insights about customer's purchasing behavior and preferred channels for purchasing products. We can also see that the High, Medium and Low income groups have different age groups and preferences, which was not evident in K=3. So, **we can choose K=5.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Buid K-Means using K=4**"
      ],
      "metadata": {
        "id": "yao7BfyKl5NZ"
      },
      "id": "yao7BfyKl5NZ"
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(data_pca)\n",
        "data_pca[\"K_means_segments_4\"] = kmeans.labels_\n",
        "data_model[\"K_means_segments_4\"] = kmeans.labels_\n",
        "data2[\"K_means_segments_4\"] = kmeans.labels_"
      ],
      "metadata": {
        "id": "b5AhrgUomKG5"
      },
      "id": "b5AhrgUomKG5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_means = data2.select_dtypes(include=np.number).groupby('K_means_segments_4').mean()\n",
        "cluster_means"
      ],
      "metadata": {
        "id": "QbnoDLdnl9Dm"
      },
      "id": "QbnoDLdnl9Dm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2[\"K_means_segments_4\"].value_counts()"
      ],
      "metadata": {
        "id": "CXNrBZlfmm2T"
      },
      "id": "CXNrBZlfmm2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PCA_PLOT(X, Y, PCA, cluster):\n",
        "    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)"
      ],
      "metadata": {
        "id": "Jax2iiLMm2ww"
      },
      "id": "Jax2iiLMm2ww",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_PLOT(0, 1, data_pca, \"K_means_segments_4\")"
      ],
      "metadata": {
        "id": "E6-1jerpm8Uz"
      },
      "id": "E6-1jerpm8Uz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cluster Profiling, K=4**"
      ],
      "metadata": {
        "id": "VjgTrYtYnSrz"
      },
      "id": "VjgTrYtYnSrz"
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_means = data2.groupby('K_means_segments_4').mean(numeric_only = True)\n",
        "cluster_means"
      ],
      "metadata": {
        "id": "akTu48l5nPj5"
      },
      "id": "akTu48l5nPj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_profile_KMeans_4 = data_model.groupby('K_means_segments_4').mean()\n",
        "cluster_means.style.highlight_max(color = \"yellow\", axis = 0).highlight_min(color=\"orange\", axis = 0)"
      ],
      "metadata": {
        "id": "kqmu2z8wnc9M"
      },
      "id": "kqmu2z8wnc9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']"
      ],
      "metadata": {
        "id": "7l5OAIgAofsu"
      },
      "id": "7l5OAIgAofsu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data_model' and 'data_pca' are already defined and populated.\n",
        "# Also assuming 'col_for_box' is defined.\n",
        "\n",
        "col_for_box = ['Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "all_col = col_for_box\n",
        "\n",
        "plt.figure(figsize=(30, 50))\n",
        "\n",
        "for i, variable in enumerate(all_col):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    sns.boxplot(y=data2[variable], x=data_pca['K_means_segments_4'], showmeans=True) # Use data_model for the variable values\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rzlL5-IgojDl"
      },
      "id": "rzlL5-IgojDl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfmtGyYGpJsV"
      },
      "id": "JfmtGyYGpJsV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e598e6d5",
      "metadata": {
        "id": "e598e6d5"
      },
      "source": [
        "## **K-Medoids**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1369fd9",
      "metadata": {
        "id": "a1369fd9"
      },
      "source": [
        "**Let's find the silhouette score for K=5 in K-Medoids**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: kmedo = ____________           # Initializing K-Medoids with number of clusters as 5 and random_state=1\n",
        "# preds = ___________            # Fit and predict K-Medoids using data_pca\n",
        "# score = ____________           # Calculate the silhouette score\n",
        "# print(score)                   # Print the score\n",
        "\n",
        "kmedo = KMedoids(n_clusters=5, random_state=1)\n",
        "preds = kmedo.fit_predict(data_pca)\n",
        "score = silhouette_score(data_pca, preds)\n",
        "score\n"
      ],
      "metadata": {
        "id": "sDFlqRk9WnyW"
      },
      "id": "sDFlqRk9WnyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fa4c3698",
      "metadata": {
        "id": "fa4c3698"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Predicting on data_pca and ddding K-Medoids cluster labels to the whole data\n",
        "# # Predicting on data_pca and ddding K-Medoids cluster labels to data_model\n",
        "# # Predicting on data_pca and ddding K-Medoids cluster labels to data_pca\n",
        "\n",
        "# Predicting on data_pca and adding K-Medoids cluster labels\n",
        "kmedo = KMedoids(n_clusters=5, random_state=1)\n",
        "kmedo.fit(data_pca)\n",
        "\n",
        "data_pca[\"K_medoids_segments_5\"] = kmedo.labels_\n",
        "data_model[\"K_medoids_segments_5\"] = kmedo.labels_\n",
        "data2[\"K_medoids_segments_5\"] = kmedo.labels_\n"
      ],
      "metadata": {
        "id": "Lg4Tua6HW87O"
      },
      "id": "Lg4Tua6HW87O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a3695d",
      "metadata": {
        "id": "50a3695d"
      },
      "outputs": [],
      "source": [
        "# Predicting on data_pca and ddding K-Medoids cluster labels to the whole data\n",
        "\n",
        "# Predicting on data_pca and ddding K-Medoids cluster labels to data_model\n",
        "\n",
        "# Predicting on data_pca and ddding K-Medoids cluster labels to data_pca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edebcc76",
      "metadata": {
        "id": "edebcc76"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n",
        "print(data2[\"K_medoids_segments_5\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d93f8763",
      "metadata": {
        "id": "d93f8763"
      },
      "source": [
        "**Let's visualize the clusters using PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f87f4d",
      "metadata": {
        "id": "93f87f4d"
      },
      "outputs": [],
      "source": [
        "# Hint: Use PCA_PLOT function created above\n",
        "def PCA_PLOT(X, Y, PCA, cluster):\n",
        "    sns.scatterplot(x=X, y=1, data=PCA, hue=cluster)\n",
        "\n",
        "PCA_PLOT(0, 1, data_pca, \"K_medoids_segments_5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SkQ8-G-XgO5"
      },
      "id": "6SkQ8-G-XgO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef5c9271",
      "metadata": {
        "id": "ef5c9271"
      },
      "source": [
        "### **Cluster Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466c8f31",
      "metadata": {
        "id": "466c8f31"
      },
      "outputs": [],
      "source": [
        "# Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_means = data2.groupby('K_medoids_segments_5').mean(numeric_only = True)\n",
        "cluster_means\n"
      ],
      "metadata": {
        "id": "g2RQ1n9hurPG"
      },
      "id": "g2RQ1n9hurPG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a594f4da",
      "metadata": {
        "id": "a594f4da"
      },
      "outputs": [],
      "source": [
        "# Highlight the maximum average value among all the clusters for each of the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_profile_KMeans_5 = data_model.groupby('K_medoids_segments_5').mean()\n",
        "cluster_means.style.highlight_max(color = \"yellow\", axis = 0).highlight_min(color=\"orange\", axis = 0)"
      ],
      "metadata": {
        "id": "w5EByjIzu59y"
      },
      "id": "w5EByjIzu59y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34ea3508",
      "metadata": {
        "id": "34ea3508"
      },
      "source": [
        "**Let's plot the boxplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23eac924",
      "metadata": {
        "id": "23eac924"
      },
      "outputs": [],
      "source": [
        "# Create boxplot for each of the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_for_box = ['Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Age','Family_Size','Expenses','NumTotalPurchases','Engaged_in_days','TotalAcceptedCmp','AmountPerPurchase']"
      ],
      "metadata": {
        "id": "go0g0jTpveHo"
      },
      "id": "go0g0jTpveHo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_for_box = ['Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "all_col = col_for_box\n",
        "\n",
        "plt.figure(figsize=(30, 50))\n",
        "\n",
        "for i, variable in enumerate(all_col):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    sns.boxplot(y=data2[variable], x=data_pca['K_medoids_segments_5'], showmeans=True) # Use data_model for the variable values\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yo1tk7MSvcBz"
      },
      "id": "Yo1tk7MSvcBz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6b7b8741",
      "metadata": {
        "id": "6b7b8741"
      },
      "source": [
        "### **Characteristics of each cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d17f83a",
      "metadata": {
        "id": "9d17f83a"
      },
      "source": [
        "**Cluster 0:__________**\n",
        "\n",
        "**Summary for cluster 0:_______________**\n",
        "\n",
        "**Cluster 1:_______________**\n",
        "\n",
        "**Summary for cluster 1:_______________**\n",
        "\n",
        "\n",
        "**Cluster 2:_______________**\n",
        "\n",
        "**Summary for cluster 2:_______________**\n",
        "\n",
        "**Cluster 3:_______________**\n",
        "\n",
        "**Summary for cluster 3:_______________**\n",
        "\n",
        "**Cluster 4:_______________**\n",
        "\n",
        "**Summary for cluster 4:_______________**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ea4ad5",
      "metadata": {
        "id": "62ea4ad5"
      },
      "outputs": [],
      "source": [
        "# Dropping labels we got from K-Medoids since we will be using PCA data for prediction\n",
        "# Hint: Use axis=1 and inplace=True\n",
        "data_pca._____________\n",
        "data.____________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Dropping labels we got from K-Medoids since we will be using PCA data for prediction\n",
        "# # Hint: Use axis=1 and inplace=True\n",
        "# data_pca._____________\n",
        "# data.____________\n",
        "\n",
        "data_pca.drop(\"K_medoids_segments_5\", axis=1, inplace=True)\n",
        "data_model.drop(\"K_medoids_segments_5\", axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "yvpH-WaZiOBR"
      },
      "id": "yvpH-WaZiOBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "424dd661",
      "metadata": {
        "id": "424dd661"
      },
      "source": [
        "## **Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020ceab2",
      "metadata": {
        "id": "020ceab2"
      },
      "source": [
        "Let's find the Cophenetic correlation for different distances with different linkage methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb02ef61",
      "metadata": {
        "id": "eb02ef61"
      },
      "source": [
        "### **What is a Cophenetic correlation?**\n",
        "\n",
        "The cophenetic correlation coefficient is a correlation coefficient between the cophenetic distances(Dendrogramic distance) obtained from the tree, and the original distances used to construct the tree. It is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points.\n",
        "\n",
        "The cophenetic distance between two observations is represented in a dendrogram by the height of the link at which those two observations are first joined. That height is the distance between the two subclusters that are merged by that link.\n",
        "\n",
        "Cophenetic correlation is the way to compare two or more dendrograms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c25567",
      "metadata": {
        "id": "26c25567"
      },
      "source": [
        "**Let's calculate Cophenetic correlation for each of the distance metrics with each of the linkage methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c1c251",
      "metadata": {
        "id": "17c1c251"
      },
      "outputs": [],
      "source": [
        "# list of distance metrics\n",
        "distance_metrics = [\"euclidean\", \"chebyshev\", \"mahalanobis\", \"cityblock\"]\n",
        "\n",
        "# list of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\"]\n",
        "\n",
        "high_cophenet_corr = 0                                                 # Creating a variable by assigning 0 to it\n",
        "high_dm_lm = [0, 0]                                                    # Creating a list by assigning 0's to it\n",
        "\n",
        "for dm in distance_metrics:\n",
        "    for lm in linkage_methods:\n",
        "        Z = linkage(data_pca, metric=dm, method=lm)                    # Applying different linkages with different distance on data_pca\n",
        "        c, coph_dists = cophenet(Z, pdist(data_pca))                   # Calculating cophenetic correlation\n",
        "        print(\n",
        "            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n",
        "                dm.capitalize(), lm, c\n",
        "            )\n",
        "        )\n",
        "        if high_cophenet_corr < c:                                     # Checking if cophenetic correlation is higher than previous score\n",
        "            high_cophenet_corr = c                                     # Appending to high_cophenet_corr list if it is higher\n",
        "            high_dm_lm[0] = dm                                         # Appending its corresponding distance\n",
        "            high_dm_lm[1] = lm                                         # Appending its corresponding method or linkage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f0c62e",
      "metadata": {
        "id": "e7f0c62e"
      },
      "outputs": [],
      "source": [
        "# Printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
        "print(\n",
        "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.\".format(\n",
        "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8c31bb",
      "metadata": {
        "id": "8c8c31bb"
      },
      "source": [
        "**Let's have a look at the dendrograms for different linkages with `Cityblock distance`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\"]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through linkage methods\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(data_pca, metric=\"cityblock\", method=method)  # Ensure metric matches cophenet\n",
        "\n",
        "    # Compute cophenetic correlation\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric=\"cityblock\"))\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram(\n",
        "        Z, ax=axs[i], truncate_mode=\"lastp\", p=12, show_leaf_counts=True\n",
        "    )\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\\nCophenetic Correlation: {coph_corr:.2f}\")\n",
        "    axs[i].set_ylabel(\"Distance\")\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pE3kQS_kjK4e"
      },
      "id": "pE3kQS_kjK4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1cc8787a",
      "metadata": {
        "id": "1cc8787a"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azAvQ4B2zsbP"
      },
      "id": "azAvQ4B2zsbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35d8fbb6",
      "metadata": {
        "id": "35d8fbb6"
      },
      "source": [
        "**Think about it:**\n",
        "\n",
        "- Can we clearly decide the number of clusters based on where to cut the dendrogram horizontally?\n",
        "- What is the next step in obtaining number of clusters based on the dendrogram?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d39f40",
      "metadata": {
        "id": "13d39f40"
      },
      "source": [
        "**Let's have a look at the dendrograms for different linkages with `Chebyshev distance`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26690933",
      "metadata": {
        "id": "26690933"
      },
      "outputs": [],
      "source": [
        "# Plot the dendrogram for Chebyshev distance with linkages single, complete and average.\n",
        "# Hint: Use Chebyshev distance as the metric in the linkage() function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\"]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through linkage methods\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(data_pca, metric=\"cityblock\", method=method)  # Ensure metric matches cophenet\n",
        "\n",
        "    # Compute cophenetic correlation\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric=\"cityblock\"))\n",
        "\n",
        "    # Determine color threshold (set dynamically at 70% of max cluster distance)\n",
        "    color_threshold = 0.7 * max(Z[:, 2])\n",
        "\n",
        "    # Plot dendrogram with colors\n",
        "    dendrogram(\n",
        "        Z,\n",
        "        ax=axs[i],\n",
        "        truncate_mode=\"lastp\",\n",
        "        p=12,\n",
        "        show_leaf_counts=True,\n",
        "        color_threshold=color_threshold,  # Apply color threshold\n",
        "        above_threshold_color='gray'  # Ensures higher branches remain gray for clarity\n",
        "    )\n",
        "\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\\nCophenetic Correlation: {coph_corr:.2f}\")\n",
        "    axs[i].set_ylabel(\"Distance\")\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_fuqTpiV0Fk5"
      },
      "id": "_fuqTpiV0Fk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "91dc3441",
      "metadata": {
        "id": "91dc3441"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29addff5",
      "metadata": {
        "id": "29addff5"
      },
      "source": [
        "**Let's have a look at the dendrograms for different linkages with Mahalanobis distance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ba170a",
      "metadata": {
        "id": "60ba170a"
      },
      "outputs": [],
      "source": [
        "# Plot the dendrogram for Mahalanobis distance with linkages single, complete and average.\n",
        "# Hint: Use Mahalanobis distance as the metric in the linkage() function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\"]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through linkage methods\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(data_pca, metric=\"mahalanobis\", method=method)  # Ensure metric matches cophenet\n",
        "\n",
        "    # Compute cophenetic correlation\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric=\"mahalanobis\"))\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram(\n",
        "        Z, ax=axs[i], truncate_mode=\"lastp\", p=12, show_leaf_counts=True\n",
        "    )\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\\nCophenetic Correlation: {coph_corr:.2f}\")\n",
        "    axs[i].set_ylabel(\"Distance\")\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Kq-12HjddPo"
      },
      "id": "9Kq-12HjddPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ebc24b3c",
      "metadata": {
        "id": "ebc24b3c"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZVFWR3HnSfL"
      },
      "id": "dZVFWR3HnSfL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad8d76b8",
      "metadata": {
        "id": "ad8d76b8"
      },
      "source": [
        "**Let's have a look at the dendrograms for different linkages with Euclidean distance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5453b59e",
      "metadata": {
        "id": "5453b59e"
      },
      "outputs": [],
      "source": [
        "# List of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\"]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(12, 18))\n",
        "\n",
        "# Iterate through linkage methods\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(data_pca, metric=\"Euclidean\", method=method)  # Ensure metric matches cophenet\n",
        "\n",
        "    # Compute cophenetic correlation\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric=\"Euclidean\"))\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram(\n",
        "        Z, ax=axs[i], truncate_mode=\"lastp\", p=12, show_leaf_counts=True\n",
        "    )\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\\nCophenetic Correlation: {coph_corr:.2f}\")\n",
        "    axs[i].set_ylabel(\"Distance\")\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, cophenet, fcluster\n",
        "\n",
        "# List of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\", \"ward\"]  # Added 'ward' linkage\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(12, 24))\n",
        "\n",
        "# Iterate through linkage methods\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(data_pca, metric=\"cityblock\", method=method)  # Ensure metric matches cophenet\n",
        "\n",
        "    # Compute cophenetic correlation\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(data_pca, metric=\"cityblock\"))\n",
        "\n",
        "    # Determine color threshold (set dynamically at 70% of max cluster distance)\n",
        "    color_threshold = 0.7 * max(Z[:, 2])\n",
        "\n",
        "    # Extract cluster labels (example: 4 clusters)\n",
        "    cluster_labels = fcluster(Z, t=4, criterion='maxclust')\n",
        "\n",
        "    # Plot dendrogram with colors\n",
        "    dendrogram(\n",
        "        Z,\n",
        "        ax=axs[i],\n",
        "        truncate_mode=\"lastp\",\n",
        "        p=12,\n",
        "        show_leaf_counts=True,\n",
        "        color_threshold=color_threshold,  # Apply color threshold\n",
        "        above_threshold_color='gray'  # Ensures higher branches remain gray for clarity\n",
        "    )\n",
        "\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\\nCophenetic Correlation: {coph_corr:.2f}\")\n",
        "    axs[i].set_ylabel(\"Distance\")\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AJxuICjg0m7I"
      },
      "id": "AJxuICjg0m7I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "24ac3177",
      "metadata": {
        "id": "24ac3177"
      },
      "source": [
        "**Think about it:**\n",
        "\n",
        "- Are there any distinct clusters in any of the dendrograms?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "017f94b8",
      "metadata": {
        "id": "017f94b8"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0caef1d",
      "metadata": {
        "id": "b0caef1d"
      },
      "outputs": [],
      "source": [
        "# Initialize Agglomerative Clustering with affinity (distance) as Euclidean, linkage as 'Ward' with clusters=3\n",
        "HCmodel = AgglomerativeClustering(n_clusters=______, affinity=______, linkage=______,)\n",
        "\n",
        "# Fit on data_pca\n",
        "HCmodel.__________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "# Convert all column names to strings\n",
        "data_pca.columns = data_pca.columns.astype(str)\n",
        "\n",
        "# Initialize Agglomerative Clustering with Euclidean distance and 'average' linkage\n",
        "HCmodel = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
        "\n",
        "# Fit the model on data_pca\n",
        "HCmodel.fit(data_pca)"
      ],
      "metadata": {
        "id": "ql5PRe32qZQu"
      },
      "id": "ql5PRe32qZQu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e893ba",
      "metadata": {
        "id": "65e893ba"
      },
      "outputs": [],
      "source": [
        "# Add Agglomerative Clustering cluster labels to data_pca\n",
        "data_pca[\"AgglomerativeClustering\"] = HCmodel.labels_\n",
        "# Add Agglomerative Clustering cluster labels to the whole data\n",
        "data2[\"AgglomerativeClustering\"] = HCmodel.labels_\n",
        "# Add Agglomerative Clustering cluster labels to data_model\n",
        "data_model[\"AgglomerativeClustering\"] = HCmodel.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ec7d8c",
      "metadata": {
        "id": "18ec7d8c"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n",
        "data2[\"AgglomerativeClustering\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2fab854",
      "metadata": {
        "id": "e2fab854"
      },
      "source": [
        "**Let's visualize the clusters using PCA.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87baf95",
      "metadata": {
        "id": "c87baf95"
      },
      "source": [
        "### **Cluster Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PCA_PLOT(X, Y, PCA, cluster):\n",
        "    sns.scatterplot(x=X, y=1, data=data_pca, hue=cluster)"
      ],
      "metadata": {
        "id": "_uUxtQfItN9E"
      },
      "id": "_uUxtQfItN9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=data_pca.iloc[:, 0],  # First Principal Component\n",
        "    y=data_pca.iloc[:, 1],  # Second Principal Component\n",
        "    hue=data_pca[\"AgglomerativeClustering\"],  # Color by cluster label\n",
        "    palette=\"viridis\",  # Use a better color palette\n",
        "    alpha=0.7,  # Adjust transparency for better visibility\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "plt.title(\"Agglomerative Clustering Scatter Plot (PCA Components)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4NcMjlX7tSDI"
      },
      "id": "4NcMjlX7tSDI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a329ff4d",
      "metadata": {
        "id": "a329ff4d"
      },
      "outputs": [],
      "source": [
        "# Select only numeric columns for calculating the mean\n",
        "numeric_cols = data2.select_dtypes(include=['number'])\n",
        "\n",
        "# Add the cluster labels to the numeric dataframe\n",
        "numeric_cols[\"AgglomerativeClustering\"] = data2[\"AgglomerativeClustering\"]\n",
        "\n",
        "# Compute the cluster-wise mean for numeric variables\n",
        "agglo_cluster_means = numeric_cols.groupby(\"AgglomerativeClustering\").mean()\n",
        "\n",
        "# Display the cluster-wise mean table\n",
        "print(agglo_cluster_means)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Highlight the min and max values for each column\n",
        "def highlight_min_max(s):\n",
        "    is_max = s == s.max()\n",
        "    is_min = s == s.min()\n",
        "    return ['background-color: yellow' if v else 'background-color: orange' if w else '' for v, w in zip(is_max, is_min)]\n",
        "\n",
        "# Apply the styling to the cluster-wise mean table\n",
        "styled_table = agglo_cluster_means.style.apply(highlight_min_max, axis=0)\n",
        "\n",
        "# Display the styled table\n",
        "styled_table\n"
      ],
      "metadata": {
        "id": "oE-IHdlvxem7"
      },
      "id": "oE-IHdlvxem7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_bBwUAStNWY"
      },
      "id": "Q_bBwUAStNWY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f934b1",
      "metadata": {
        "id": "a7f934b1"
      },
      "outputs": [],
      "source": [
        "# Highlight the maximum average value among all the clusters for each of the variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52260d8a",
      "metadata": {
        "id": "52260d8a"
      },
      "source": [
        "**Let's plot the boxplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a60a12e",
      "metadata": {
        "id": "4a60a12e"
      },
      "outputs": [],
      "source": [
        "# Create boxplot for each of the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "# Set a general style and color palette for the plots\n",
        "sns.set(style=\"whitegrid\", palette=[\"#1f77b4\"])  # Blue color\n",
        "\n",
        "# Determine the number of plots and grid size\n",
        "num_cols = len(numeric_cols.columns) - 1  # Exclude 'AgglomerativeClustering'\n",
        "cols_per_row = 3  # Adjust this number for more or fewer plots per row\n",
        "rows = math.ceil(num_cols / cols_per_row)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols_per_row, figsize=(cols_per_row * 6, rows * 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each numeric column\n",
        "plot_index = 0\n",
        "for column in numeric_cols.columns:\n",
        "    if column != \"AgglomerativeClustering\":\n",
        "        sns.boxplot(x=\"AgglomerativeClustering\", y=column, data=numeric_cols, ax=axes[plot_index], color=\"#1f77b4\")\n",
        "        axes[plot_index].set_title(f'Boxplot of {column} by Cluster', fontsize=12)\n",
        "        axes[plot_index].set_xlabel('Cluster')\n",
        "        axes[plot_index].set_ylabel(column)\n",
        "        plot_index += 1\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(plot_index, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DX3Sc0qQyH8H"
      },
      "id": "DX3Sc0qQyH8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "237ce2e9",
      "metadata": {
        "id": "237ce2e9"
      },
      "source": [
        "### **Characteristics of each cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3534a662",
      "metadata": {
        "id": "3534a662"
      },
      "source": [
        "**Cluster 0:__________**\n",
        "\n",
        "**Summary for cluster 0:_______________**\n",
        "\n",
        "**Cluster 1:_______________**\n",
        "\n",
        "**Summary for cluster 1:_______________**\n",
        "\n",
        "\n",
        "**Cluster 2:_______________**\n",
        "\n",
        "**Summary for cluster 2:_______________**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b8aa74",
      "metadata": {
        "id": "51b8aa74"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37179889",
      "metadata": {
        "id": "37179889"
      },
      "outputs": [],
      "source": [
        "# Dropping labels we got from Agglomerative Clustering since we will be using PCA data for prediction\n",
        "# Hint: Use axis=1 and inplace=True\n",
        "\n",
        "data2.drop(\"AgglomerativeClustering\", axis=1, inplace=True)\n",
        "data_pca.drop(\"AgglomerativeClustering\", axis=1, inplace=True)\n",
        "data_model.drop(\"AgglomerativeClustering\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e425520",
      "metadata": {
        "id": "8e425520"
      },
      "source": [
        "## **DBSCAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c941735",
      "metadata": {
        "id": "5c941735"
      },
      "source": [
        "DBSCAN is a very powerful algorithm for finding high-density clusters, but the problem is determining the best set of hyperparameters to use with it. It includes two hyperparameters, `eps`, and `min samples`.\n",
        "\n",
        "Since it is an unsupervised algorithm, you have no control over it, unlike a supervised learning algorithm, which allows you to test your algorithm on a validation set. The approach we can follow is basically trying out a bunch of different combinations of values and finding the silhouette score for each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480b6c72",
      "metadata": {
        "id": "480b6c72"
      },
      "outputs": [],
      "source": [
        "# Initializing lists\n",
        "eps_value = [2,3]                       # Taking random eps value\n",
        "min_sample_values = [6,20]              # Taking random min_sample value\n",
        "\n",
        "# Creating a dictionary for each of the values in eps_value with min_sample_values\n",
        "res = {eps_value[i]: min_sample_values for i in range(len(eps_value))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c5e7e6",
      "metadata": {
        "id": "c0c5e7e6"
      },
      "outputs": [],
      "source": [
        "# Finding the silhouette_score for each of the combinations\n",
        "high_silhouette_avg = 0                                               # Assigning 0 to the high_silhouette_avg variable\n",
        "high_i_j = [0, 0]                                                     # Assigning 0's to the high_i_j list\n",
        "key = res.keys()                                                      # Assigning dictionary keys to a variable called key\n",
        "for i in key:\n",
        "    z = res[i]                                                        # Assigning dictionary values of each i to z\n",
        "    for j in z:\n",
        "        db = DBSCAN(eps=i, min_samples=j).fit(data_pca)               # Applying DBSCAN to each of the combination in dictionary\n",
        "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "        core_samples_mask[db.core_sample_indices_] = True\n",
        "        labels = db.labels_\n",
        "        silhouette_avg = silhouette_score(data_pca, labels)           # Finding silhouette score\n",
        "        print(\n",
        "            \"For eps value =\" + str(i),\n",
        "            \"For min sample =\" + str(j),\n",
        "            \"The average silhoutte_score is :\",\n",
        "            silhouette_avg,                                          # Printing the silhouette score for each of the combinations\n",
        "        )\n",
        "        if high_silhouette_avg < silhouette_avg:                     # If the silhouette score is greater than 0 or the previous score, it will get appended to the high_silhouette_avg list with its combination of i and j\n",
        "            high_i_j[0] = i\n",
        "            high_i_j[1] = j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ce94a1",
      "metadata": {
        "id": "50ce94a1"
      },
      "outputs": [],
      "source": [
        "# Printing the highest silhouette score\n",
        "print(\"Highest_silhoutte_avg is {} for eps = {} and min sample = {}\".format(high_silhouette_avg, high_i_j[0], high_i_j[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80247dce",
      "metadata": {
        "id": "80247dce"
      },
      "source": [
        "**Now, let's apply DBSCAN using the hyperparameter values we have received above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d118ef9e",
      "metadata": {
        "id": "d118ef9e"
      },
      "outputs": [],
      "source": [
        "# Apply DBSCAN using the above hyperparameter values\n",
        "dbs = _____________________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "uU0VcS2n8mCH"
      },
      "id": "uU0VcS2n8mCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the number of min_samples for k\n",
        "k = 20\n",
        "neighbors = NearestNeighbors(n_neighbors=k)\n",
        "neighbors_fit = neighbors.fit(data_pca)\n",
        "distances, indices = neighbors_fit.kneighbors(data_pca)\n",
        "\n",
        "# Sort and plot the distances\n",
        "distances = np.sort(distances[:, k-1], axis=0)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(distances)\n",
        "plt.title('k-distance Graph for DBSCAN (k=20)')\n",
        "plt.xlabel('Data Points sorted by distance')\n",
        "plt.ylabel(f'{k}th Nearest Neighbor Distance')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pxglnVbm90Ha"
      },
      "id": "pxglnVbm90Ha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "for min_samples in [5, 10, 15]:\n",
        "    dbscan = DBSCAN(eps=3, min_samples=min_samples, metric='euclidean')\n",
        "    labels = dbscan.fit_predict(data_pca)\n",
        "    print(f'min_samples={min_samples} => Clusters found: {len(set(labels)) - (1 if -1 in labels else 0)}')\n"
      ],
      "metadata": {
        "id": "lDOfb_OO99tH"
      },
      "id": "lDOfb_OO99tH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_values = [2, 3, 4, 5]\n",
        "min_samples_values = [5, 10, 15, 20]\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "        labels = dbscan.fit_predict(data_pca)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if n_clusters > 1:\n",
        "            score = silhouette_score(data_pca, labels)\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Clusters={n_clusters}, Silhouette={score:.4f}')\n",
        "        else:\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Only {n_clusters} cluster found.')\n"
      ],
      "metadata": {
        "id": "RwC__0Qn-Emg"
      },
      "id": "RwC__0Qn-Emg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data_model)  # Assuming data_model is the unscaled behavioral data\n"
      ],
      "metadata": {
        "id": "QB88R642-O4e"
      },
      "id": "QB88R642-O4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5)  # Try more components\n",
        "data_pca = pca.fit_transform(data_model)\n"
      ],
      "metadata": {
        "id": "h3Y738_5-Yl_"
      },
      "id": "h3Y738_5-Yl_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "cJ01Jwok-xuW"
      },
      "id": "cJ01Jwok-xuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=5, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "XT4ii8lm-86P"
      },
      "id": "XT4ii8lm-86P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "6o849CFA-gpP"
      },
      "id": "6o849CFA-gpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa911be2",
      "metadata": {
        "id": "fa911be2"
      },
      "outputs": [],
      "source": [
        "# fit_predict on data_pca and add DBSCAN cluster labels to the whole data\n",
        "\n",
        "# fit_predict on data_pca and add DBSCAN cluster labels to data_model\n",
        "\n",
        "# fit_predict on data_pca and add DBSCAN cluster labels to data_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47699c3e",
      "metadata": {
        "id": "47699c3e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a311c74",
      "metadata": {
        "id": "8a311c74"
      },
      "source": [
        "**Let's visualize the clusters using PCA.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426e7f93",
      "metadata": {
        "id": "426e7f93"
      },
      "outputs": [],
      "source": [
        "# Hint: Use PCA_PLOT function created above\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=data_pca[:,0], y=data_pca[:,1], hue=data2['DBSCAN_Cluster'], palette='tab10')\n",
        "plt.title('DBSCAN Clustering Visualization (Adjusted Parameters)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8gGiQyy3-c4O"
      },
      "id": "8gGiQyy3-c4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5)  # Increase from previous 2 to 5\n",
        "data_pca = pca.fit_transform(data_model)  # Assuming data_model is already scaled\n"
      ],
      "metadata": {
        "id": "QCGbPY2rAPW3"
      },
      "id": "QCGbPY2rAPW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "S8Jq6GSgAQbZ"
      },
      "id": "S8Jq6GSgAQbZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=4)  # Increase from previous 2 to 5\n",
        "data_pca = pca.fit_transform(data_model)  # Assuming data_model is already scaled"
      ],
      "metadata": {
        "id": "KQXN5JuFCZ9h"
      },
      "id": "KQXN5JuFCZ9h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "0scGNVRvCm1b"
      },
      "id": "0scGNVRvCm1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=6)  # Increase from previous 2 to 5\n",
        "data_pca = pca.fit_transform(data_model)  # Assuming data_model is already scaled"
      ],
      "metadata": {
        "id": "_Z-6lFZgCSr8"
      },
      "id": "_Z-6lFZgCSr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "vGo0bi6-CW4-"
      },
      "id": "vGo0bi6-CW4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data_model = scaler.fit_transform(data_model)  # Use data_model for consistency\n",
        "\n",
        "pca = PCA(n_components=5)  # Try with more components again\n",
        "data_pca = pca.fit_transform(scaled_data_model)"
      ],
      "metadata": {
        "id": "Dz4iazwUAkav"
      },
      "id": "Dz4iazwUAkav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "1OaNtipXAkNM"
      },
      "id": "1OaNtipXAkNM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_values = [2, 3, 4, 5, 6]\n",
        "min_samples_values = [5, 10, 15, 20]\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "        labels = dbscan.fit_predict(data_pca)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if n_clusters > 1:\n",
        "            score = silhouette_score(data_pca, labels)\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Clusters={n_clusters}, Silhouette={score:.4f}')\n",
        "        else:\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Only {n_clusters} cluster(s) found.')\n"
      ],
      "metadata": {
        "id": "Pv-eE8P5Ay-B"
      },
      "id": "Pv-eE8P5Ay-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10')\n",
        "plt.title('DBSCAN Clustering Visualization (After Adjustments)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rrv85FSIA2oQ"
      },
      "id": "rrv85FSIA2oQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Recompute PCA with 10 components\n",
        "pca = PCA(n_components=10)\n",
        "data_pca = pca.fit_transform(data_model)  # Assuming data_model is scaled\n",
        "\n",
        "# Apply DBSCAN with broader parameter tuning\n",
        "eps_values = [1, 2, 3, 5, 10]\n",
        "min_samples_values = [5, 10, 15, 20, 30]\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "        labels = dbscan.fit_predict(data_pca)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if n_clusters > 1:\n",
        "            score = silhouette_score(data_pca, labels)\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Clusters={n_clusters}, Silhouette={score:.4f}')\n",
        "        else:\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Only {n_clusters} cluster(s).')\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10')\n",
        "plt.title('DBSCAN Clustering Visualization (10 PCA Components)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ud7f3vKFDTyp"
      },
      "id": "ud7f3vKFDTyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=10)  # Try more components\n",
        "pca.fit(data_model)\n",
        "print(\"Explained variance ratio per component:\", pca.explained_variance_ratio_)\n",
        "print(\"Cumulative explained variance:\", pca.explained_variance_ratio_.cumsum())\n"
      ],
      "metadata": {
        "id": "-_DlKobBBkpU"
      },
      "id": "-_DlKobBBkpU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply DBSCAN on PCA-transformed data (data_pca)\n",
        "dbscan = DBSCAN(eps=3, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# Add DBSCAN cluster labels to data2 for profiling\n",
        "data2['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Calculate silhouette score (only valid if multiple clusters and no noise-only clusters)\n",
        "if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
        "    score = silhouette_score(data_pca, dbscan_labels)\n",
        "    print(f\"Silhouette Score for DBSCAN (eps=3, min_samples=20): {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score could not be calculated due to insufficient distinct clusters or presence of noise (-1).\")\n",
        "\n",
        "# View the distribution of points per cluster (including noise labeled as -1)\n",
        "print(data2['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "RD5qT1E4C2qE"
      },
      "id": "RD5qT1E4C2qE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data_model = scaler.fit_transform(data_model)\n",
        "\n",
        "pca = PCA(n_components=5)  # Adjust based on variance check\n",
        "data_pca = pca.fit_transform(scaled_data_model)\n"
      ],
      "metadata": {
        "id": "jA0WyHACBs2Y"
      },
      "id": "jA0WyHACBs2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_values = [1, 2, 3, 5, 10]\n",
        "min_samples_values = [5, 10, 15, 20, 30]\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "        labels = dbscan.fit_predict(data_pca)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        if n_clusters > 1:\n",
        "            score = silhouette_score(data_pca, labels)\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Clusters={n_clusters}, Silhouette={score:.4f}')\n",
        "        else:\n",
        "            print(f'eps={eps}, min_samples={min_samples} => Only {n_clusters} cluster(s).')\n"
      ],
      "metadata": {
        "id": "Ifb7C4gaBw3h"
      },
      "id": "Ifb7C4gaBw3h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10')\n",
        "plt.title('DBSCAN Clustering Visualization (Adjusted Parameters)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xz2mDRBkB3rI"
      },
      "id": "Xz2mDRBkB3rI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Drop these columns from data2 k_means_segments_3\tK_means_segments_4\tK_medoids_segments_5\tDBSCAN_Cluster\tGMM_Cluster\n",
        "\n",
        "data2 = data2.drop(columns=['k_means_segments_3', 'K_means_segments_4', 'K_medoids_segments_5', 'DBSCAN_Cluster', 'GMM_Cluster'])\n"
      ],
      "metadata": {
        "id": "S0BaDZKhKrAY"
      },
      "id": "S0BaDZKhKrAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data2.drop(columns=['k_means_segments_3', 'K_means_segments_4', 'K_medoids_segments_5', 'DBSCAN_Cluster', 'GMM_Cluster'])"
      ],
      "metadata": {
        "id": "0-K5Niu6Kx-U"
      },
      "id": "0-K5Niu6Kx-U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "UMawdeZWB3XZ"
      },
      "id": "UMawdeZWB3XZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Assuming 'data_model' is your DataFrame with behavioral features only\n",
        "\n",
        "# 1. Scale the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data_model = scaler.fit_transform(data_model)\n",
        "\n",
        "# 2. Apply PCA with 10 components (capturing ~92.82% variance)\n",
        "pca = PCA(n_components=10)\n",
        "data_pca = pca.fit_transform(scaled_data_model)\n",
        "\n",
        "# 3. Apply DBSCAN with best hyperparameters: eps=2, min_samples=20\n",
        "dbscan = DBSCAN(eps=2, min_samples=20, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(data_pca)\n",
        "\n",
        "# 4. Create a scatter plot for the first two PCA components\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=data_pca[:, 0], y=data_pca[:, 1],\n",
        "    hue=dbscan_labels, palette='tab10', legend='full'\n",
        ")\n",
        "plt.title('DBSCAN Clustering Visualization (Best Parameters: eps=2, min_samples=20)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster', loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g8ZcTZ7TGXl_"
      },
      "id": "g8ZcTZ7TGXl_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5a80374c",
      "metadata": {
        "id": "5a80374c"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de39ed10",
      "metadata": {
        "id": "de39ed10"
      },
      "source": [
        "**Think about it:**\n",
        "\n",
        "- Changing the eps and min sample values will result in different DBSCAN results? Can we try more value for eps and min_sample?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06334fb7",
      "metadata": {
        "id": "06334fb7"
      },
      "source": [
        "**Note:** You can experiment with different eps and min_sample values to see if DBSCAN produces good distribution and cluster profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0a7075",
      "metadata": {
        "id": "1f0a7075"
      },
      "outputs": [],
      "source": [
        "# Dropping labels we got from DBSCAN since we will be using PCA data for prediction\n",
        "# Hint: Use axis=1 and inplace=True\n",
        "data_pca._____________\n",
        "data.____________"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30613efa",
      "metadata": {
        "id": "30613efa"
      },
      "source": [
        "## **Gaussian Mixture Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fca82b0",
      "metadata": {
        "id": "8fca82b0"
      },
      "source": [
        "**Let's find the silhouette score for K=5 in Gaussian Mixture**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fit GMM with K=5 clusters\n",
        "gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
        "gmm_labels = gmm.fit_predict(data_pca)  # Predict cluster membership\n",
        "\n",
        "# Step 2: Calculate Silhouette Score\n",
        "silhouette_avg = silhouette_score(data_pca, gmm_labels)\n",
        "print(f'Silhouette Score for GMM with 5 clusters: {silhouette_avg:.4f}')\n",
        "\n",
        "# Step 3: Add cluster labels to data2 for profiling\n",
        "data2['GMM_Cluster'] = gmm_labels\n",
        "\n",
        "# Step 4: Visualize the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=gmm_labels, palette='tab10', legend='full')\n",
        "plt.title('GMM Clustering Visualization (K=5 Clusters)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster', loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8wcgTdQkIcc4"
      },
      "id": "8wcgTdQkIcc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "# BIC and AIC plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(n_components_range, bic_scores, label='BIC', marker='o')\n",
        "plt.plot(n_components_range, aic_scores, label='AIC', marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Score')\n",
        "plt.title('BIC & AIC for Different K Values')\n",
        "plt.legend()\n",
        "\n",
        "# ✅ Fixed Silhouette Score plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(n_components_range, silhouette_scores, label='Silhouette Score', marker='o', color='green')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different K Values')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "OJdVKCVaJaIA"
      },
      "id": "OJdVKCVaJaIA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4ade2e",
      "metadata": {
        "id": "9d4ade2e"
      },
      "outputs": [],
      "source": [
        "gmm = GaussianMixture(_______) # Initialize Gaussian Mixture Model with number of clusters as 5 and random_state=1\n",
        "\n",
        "preds = ___________            # Fit and predict Gaussian Mixture Model using data_pca\n",
        "\n",
        "score = ____________           # Calculate the silhouette score\n",
        "\n",
        "print(score)                   # Print the score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Ensure data_pca is a NumPy array\n",
        "if isinstance(data_pca, pd.DataFrame):\n",
        "    data_pca = data_pca.values  # Convert DataFrame to NumPy array\n",
        "\n",
        "# Visualize clusters (PCA components)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=gmm_final_labels, palette='tab10', legend='full')\n",
        "plt.title('GMM Clustering Visualization (K=2 Clusters)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster', loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hfo4o389LOnT"
      },
      "id": "hfo4o389LOnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for profiling\n",
        "numeric_cols = data2.select_dtypes(include=['number'])\n",
        "\n",
        "# Compute cluster-wise means for profiling\n",
        "cluster_profiles = numeric_cols.groupby('GMM_Cluster').mean().transpose()\n",
        "\n",
        "# Display the profiling results\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"GMM_Cluster_Profiles\", dataframe=cluster_profiles)\n"
      ],
      "metadata": {
        "id": "35VeDRKULpu1"
      },
      "id": "35VeDRKULpu1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cluster-wise means for profiling\n",
        "numeric_cols = data2.select_dtypes(include=['number'])\n",
        "gmm_cluster_profiles = numeric_cols.groupby('GMM_Cluster').mean().transpose()\n",
        "\n",
        "# Display profiling results using standard pandas display\n",
        "print(\"GMM Cluster Profiles (K=2):\")\n",
        "display(gmm_cluster_profiles)  # Standard display function in Jupyter\n",
        "\n",
        "# Optional: Export to CSV if you want to view in Excel\n",
        "gmm_cluster_profiles.to_csv('GMM_Cluster_Profiles_K2.csv')\n",
        "print(\"Cluster profiles saved as 'GMM_Cluster_Profiles_K2.csv'\")\n"
      ],
      "metadata": {
        "id": "VXHTw3BGMMat"
      },
      "id": "VXHTw3BGMMat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3e0c8dc6",
      "metadata": {
        "id": "3e0c8dc6"
      },
      "source": [
        "**Observations and Insights:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b4f14f",
      "metadata": {
        "id": "a1b4f14f"
      },
      "outputs": [],
      "source": [
        "# Predicting on data_pca and add Gaussian Mixture Model cluster labels to the whole data\n",
        "\n",
        "# Predicting on data_pca and add Gaussian Mixture Model cluster labels to data_model\n",
        "\n",
        "# Predicting on data_pca and add Gaussian Mixture Model cluster labels to data_pca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1f8dde",
      "metadata": {
        "id": "2c1f8dde"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642ebb6b",
      "metadata": {
        "id": "642ebb6b"
      },
      "source": [
        "**Let's visualize the clusters using PCA.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52969ce",
      "metadata": {
        "id": "c52969ce"
      },
      "outputs": [],
      "source": [
        "# Hint: Use PCA_PLOT function created above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e536e1",
      "metadata": {
        "id": "31e536e1"
      },
      "source": [
        "### **Cluster Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2fb20e",
      "metadata": {
        "id": "3a2fb20e"
      },
      "outputs": [],
      "source": [
        "# Take the cluster-wise mean of all the variables. Hint: First group 'data' by cluster labels column and then find mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280c66a7",
      "metadata": {
        "id": "280c66a7"
      },
      "outputs": [],
      "source": [
        "# Highlight the maximum average value among all the clusters for each of the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to highlight max (yellow) and min (orange) values per row\n",
        "def highlight_min_max(s):\n",
        "    is_max = s == s.max()\n",
        "    is_min = s == s.min()\n",
        "    return ['background-color: yellow' if v else 'background-color: orange' if w else ''\n",
        "            for v, w in zip(is_max, is_min)]\n",
        "\n",
        "# Apply the highlighting\n",
        "styled_profiles = gmm_cluster_profiles.style.apply(highlight_min_max, axis=1)\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_profiles\n"
      ],
      "metadata": {
        "id": "QFlw0LwqMfR3"
      },
      "id": "QFlw0LwqMfR3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Fit GMM with K=5 clusters\n",
        "gmm_5 = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
        "gmm_5_labels = gmm_5.fit_predict(data_pca)\n",
        "\n",
        "# Step 2: Add cluster labels to data2\n",
        "data2['GMM_Cluster_5'] = gmm_5_labels\n",
        "\n",
        "# Step 3: Compute cluster-wise means for profiling\n",
        "numeric_cols_5 = data2.select_dtypes(include=['number'])\n",
        "gmm_cluster_profiles_5 = numeric_cols_5.groupby('GMM_Cluster_5').mean().transpose()\n",
        "\n",
        "# Step 4: Highlight max (yellow) and min (orange) per variable\n",
        "def highlight_min_max(s):\n",
        "    is_max = s == s.max()\n",
        "    is_min = s == s.min()\n",
        "    return ['background-color: yellow' if v else 'background-color: orange' if w else ''\n",
        "            for v, w in zip(is_max, is_min)]\n",
        "\n",
        "# Apply highlighting\n",
        "styled_profiles_5 = gmm_cluster_profiles_5.style.apply(highlight_min_max, axis=1)\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_profiles_5\n"
      ],
      "metadata": {
        "id": "laLgMHd-M9yJ"
      },
      "id": "laLgMHd-M9yJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Scatter plot of the 5 clusters using the first two PCA components\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=data_pca[:, 0],\n",
        "    y=data_pca[:, 1],\n",
        "    hue=gmm_5_labels,\n",
        "    palette='tab10',\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Step 2: Customize the plot\n",
        "plt.title('PCA Scatter Plot for GMM Clustering (K=5)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster', loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8MbiSzDqNchk"
      },
      "id": "8MbiSzDqNchk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "# Step 1: Prepare the data for plotting\n",
        "numeric_features = data2.select_dtypes(include=['number']).drop(columns=['GMM_Cluster_5'])\n",
        "num_features = len(numeric_features.columns)\n",
        "\n",
        "# Step 2: Set the grid size for multiple boxplots\n",
        "cols_per_row = 3\n",
        "rows = math.ceil(num_features / cols_per_row)\n",
        "\n",
        "# Step 3: Create boxplots for each feature grouped by GMM_Cluster_5\n",
        "fig, axes = plt.subplots(rows, cols_per_row, figsize=(cols_per_row * 6, rows * 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, column in enumerate(numeric_features.columns):\n",
        "    sns.boxplot(\n",
        "        x='GMM_Cluster_5',\n",
        "        y=column,\n",
        "        data=data2,\n",
        "        ax=axes[i],\n",
        "        palette='Blues'\n",
        "    )\n",
        "    axes[i].set_title(f'Boxplot of {column} by GMM Cluster (K=5)', fontsize=10)\n",
        "    axes[i].set_xlabel('Cluster')\n",
        "    axes[i].set_ylabel(column)\n",
        "\n",
        "# Step 4: Remove empty subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S8EHxL5vN5RI"
      },
      "id": "S8EHxL5vN5RI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "72a2d324",
      "metadata": {
        "id": "72a2d324"
      },
      "source": [
        "**Let's plot the boxplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695c58f8",
      "metadata": {
        "id": "695c58f8"
      },
      "outputs": [],
      "source": [
        "# Create boxplot for each of the variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10442989",
      "metadata": {
        "id": "10442989"
      },
      "source": [
        "### **Characteristics of each cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f776b12",
      "metadata": {
        "id": "0f776b12"
      },
      "source": [
        "**Cluster 0:__________**\n",
        "\n",
        "**Summary for cluster 0:_______________**\n",
        "\n",
        "**Cluster 1:_______________**\n",
        "\n",
        "**Summary for cluster 1:_______________**\n",
        "\n",
        "\n",
        "**Cluster 2:_______________**\n",
        "\n",
        "**Summary for cluster 2:_______________**\n",
        "\n",
        "**Cluster 3:_______________**\n",
        "\n",
        "**Summary for cluster 3:_______________**\n",
        "\n",
        "**Cluster 4:_______________**\n",
        "\n",
        "**Summary for cluster 4:_______________**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73367782",
      "metadata": {
        "id": "73367782"
      },
      "source": [
        "## **Conclusion and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N5BT7Ocwqf5x",
      "metadata": {
        "id": "N5BT7Ocwqf5x"
      },
      "source": [
        "**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success)**:\n",
        "- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjc6vTcoqp6v",
      "metadata": {
        "id": "wjc6vTcoqp6v"
      },
      "source": [
        "**2. Refined insights**:\n",
        "- What are the most meaningful insights from the data relevant to the problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hK6PMGUtoxVx",
      "metadata": {
        "id": "hK6PMGUtoxVx"
      },
      "source": [
        "**3. Proposal for the final solution design:**\n",
        "- What model do you propose to be adopted? Why is this the best solution to adopt?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=kmeans_labels, palette='tab10', legend='full')\n",
        "plt.title('PCA Scatter Plot: K-Means (K=4) Clustering')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hjGJF9y6OcrB"
      },
      "id": "hjGJF9y6OcrB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}